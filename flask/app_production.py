"""
VPN Enterprise - Production-Ready AI API
Powered by OpenAI & Anthropic - MORE POWERFUL than Cursor/Lovable
Scalable FastAPI microservice with authentication, rate limiting, and caching
"""
from fastapi import FastAPI, HTTPException, status, Depends, Header, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, ValidationError
from datetime import datetime, timedelta
from contextlib import asynccontextmanager
from typing import Optional, Dict, Any, List
import httpx
import os
import logging
import hashlib
import json
import time
import asyncio
from functools import wraps

# AI Provider imports - Professional grade APIs
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from app_deployment import AppDeploymentService

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# AI Provider Configuration
AI_PROVIDER = os.getenv("AI_PROVIDER", "openai")  # openai or anthropic
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")

# Initialize AI clients
openai_client = None
anthropic_client = None

if OPENAI_API_KEY:
    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    logger.info("âœ… OpenAI client initialized")
    
if ANTHROPIC_API_KEY:
    anthropic_client = AsyncAnthropic(api_key=ANTHROPIC_API_KEY)
    logger.info("âœ… Anthropic client initialized")

# Initialize deployment service
deployment_service = AppDeploymentService()
logger.info("âœ… App deployment service initialized")

# Service discovery
SERVICES = {
    "api": os.getenv("API_URL", "http://vpn-api:5000"),
    "web": os.getenv("WEB_URL", "http://vpn-web:3000"),
    "redis_host": os.getenv("REDIS_HOST", "vpn-redis"),
    "redis_port": int(os.getenv("REDIS_PORT", "6379")),
    "n8n": os.getenv("N8N_URL", "http://vpn-n8n:5678"),
    "postgres": os.getenv("POSTGRES_URL", "postgresql://postgres:postgres@vpn-postgres:5432/postgres")
}

# In-memory cache (fallback if Redis unavailable)
memory_cache: Dict[str, Any] = {}
rate_limit_store: Dict[str, List[float]] = {}

# Configuration
JWT_SECRET = os.getenv("JWT_SECRET", "your-secret-key-CHANGE-THIS-IN-PRODUCTION")
CACHE_TTL = int(os.getenv("CACHE_TTL", "3600"))  # 1 hour default

# Rate limits by tier
RATE_LIMITS = {
    "free": {"requests": 100, "window": 3600},
    "pro": {"requests": 1000, "window": 3600},
    "enterprise": {"requests": 10000, "window": 3600},
    "unlimited": {"requests": 1000000, "window": 3600}
}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler"""
    logger.info("ðŸš€ VPN Enterprise AI API Starting (Production Mode)...")
    logger.info(f"ðŸ¤– AI Provider: {AI_PROVIDER.upper()}")
    logger.info(f"ðŸ“¡ {len(SERVICES)} backend services configured")
    for name, url in SERVICES.items():
        if isinstance(url, str):
            logger.info(f"   â€¢ {name}: {url}")
    
    if not openai_client and not anthropic_client:
        logger.warning("âš ï¸  NO AI API KEYS SET! Set OPENAI_API_KEY or ANTHROPIC_API_KEY")
    
    yield
    
    logger.info("ðŸ›‘ Shutting down gracefully...")

app = FastAPI(
    title="VPN Enterprise AI API",
    description="Production AI microservice powered by OpenAI GPT-4 & Anthropic Claude - MORE POWERFUL than Cursor/Lovable",
    version="3.0.0",
    lifespan=lifespan
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# EXCEPTION HANDLERS
# ============================================

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    \"\"\"Handle validation errors with detailed messages\"\"\"
    errors = exc.errors()
    logger.error(f\"ðŸš¨ Validation Error on {request.method} {request.url.path}:\")\n    logger.error(f\"   Request body: {await request.body()}\")\n    logger.error(f\"   Errors: {json.dumps(errors, indent=2)}\")\n    \n    # Create user-friendly error message\n    error_details = []\n    for error in errors:\n        field = \" -> \".join(str(loc) for loc in error[\"loc\"])\n        msg = error[\"msg\"]\n        error_details.append(f\"{field}: {msg}\")\n    \n    return JSONResponse(\n        status_code=422,\n        content={\n            \"detail\": \"Request validation failed\",\n            \"errors\": error_details,\n            \"raw_errors\": errors\n        }\n    )

# ============================================
# UTILITIES
# ============================================

def generate_cache_key(prefix: str, **kwargs) -> str:
    """Generate consistent cache key"""
    key_parts = [prefix] + [f"{k}:{v}" for k, v in sorted(kwargs.items())]
    key_string = ":".join(key_parts)
    return hashlib.md5(key_string.encode()).hexdigest()

async def get_from_cache(key: str) -> Optional[Any]:
    """Get value from cache"""
    try:
        if key in memory_cache:
            cached = memory_cache[key]
            if cached["expires"] > time.time():
                return cached["data"]
            else:
                del memory_cache[key]
    except Exception as e:
        logger.error(f"Cache get error: {e}")
    return None

async def set_in_cache(key: str, value: Any, ttl: int = CACHE_TTL):
    """Set value in cache"""
    try:
        memory_cache[key] = {
            "data": value,
            "expires": time.time() + ttl
        }
    except Exception as e:
        logger.error(f"Cache set error: {e}")

async def check_rate_limit(identifier: str, tier: str = "free") -> Dict[str, Any]:
    """Check if request is within rate limit"""
    limit_config = RATE_LIMITS.get(tier, RATE_LIMITS["free"])
    max_requests = limit_config["requests"]
    window = limit_config["window"]
    
    current_time = time.time()
    key = f"ratelimit:{identifier}"
    
    if key not in rate_limit_store:
        rate_limit_store[key] = []
    
    # Remove expired timestamps
    rate_limit_store[key] = [
        ts for ts in rate_limit_store[key]
        if current_time - ts < window
    ]
    
    requests_used = len(rate_limit_store[key])
    remaining = max_requests - requests_used
    
    if remaining <= 0:
        reset_time = min(rate_limit_store[key]) + window if rate_limit_store[key] else current_time + window
        return {
            "allowed": False,
            "requests_used": requests_used,
            "requests_limit": max_requests,
            "requests_remaining": 0,
            "window_reset": datetime.fromtimestamp(reset_time)
        }
    
    # Add current request
    rate_limit_store[key].append(current_time)
    
    return {
        "allowed": True,
        "requests_used": requests_used + 1,
        "requests_limit": max_requests,
        "requests_remaining": remaining - 1,
        "window_reset": datetime.fromtimestamp(current_time + window)
    }

async def verify_token(
    authorization: Optional[str] = Header(None),
    x_api_key: Optional[str] = Header(None, alias="X-API-Key")
) -> Dict[str, Any]:
    """Verify JWT token or API key (supports both Authorization and X-API-Key headers)"""
    
    # Try X-API-Key header first (preferred by NexusAI)
    if x_api_key:
        if x_api_key.startswith("vpn_"):
            return {
                "tier": "pro",
                "user_id": hashlib.md5(x_api_key.encode()).hexdigest()[:8],
                "tenant_id": "nexusai"
            }
        else:
            raise HTTPException(status_code=401, detail="Invalid API key format")
    
    # Fall back to Authorization header
    if not authorization:
        return {"tier": "free", "user_id": "anonymous"}
    
    try:
        if authorization.startswith("Bearer "):
            token = authorization.replace("Bearer ", "")
            # Check if it's an API key
            if token.startswith("vpn_"):
                return {
                    "tier": "pro",
                    "user_id": hashlib.md5(token.encode()).hexdigest()[:8],
                    "tenant_id": "nexusai"
                }
            # Otherwise it's a JWT token
            return {
                "tier": "pro",
                "user_id": token[:8],
                "tenant_id": "default"
            }
        else:
            # Legacy: direct API key without Bearer
            if authorization.startswith("vpn_"):
                return {
                    "tier": "pro",
                    "user_id": hashlib.md5(authorization.encode()).hexdigest()[:8],
                    "tenant_id": "nexusai"
                }
            return {"tier": "free", "user_id": "anonymous"}
    except Exception as e:
        logger.error(f"Token verification failed: {e}")
        return {"tier": "free", "user_id": "anonymous"}

# ============================================
# MODELS
# ============================================

class HealthResponse(BaseModel):
    status: str
    timestamp: datetime
    service: str
    version: str
    environment: str

class AIRequest(BaseModel):
    prompt: str = Field(..., min_length=1, max_length=4000)
    model: str = Field(default="llama3.2:1b")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    context: Optional[str] = None
    use_cache: bool = Field(default=True)

class AIResponse(BaseModel):
    response: str
    model: str
    cached: bool = False
    eval_count: Optional[int] = None
    total_duration_ms: Optional[float] = None

class SQLAssistRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=2000)
    database_schema: Optional[str] = Field(None, alias="schema")
    action: str = Field(default="generate", pattern="^(generate|explain|optimize|fix)$")

class SQLAssistResponse(BaseModel):
    sql: Optional[str] = None
    explanation: Optional[str] = None
    suggestions: Optional[List[str]] = None
    cached: bool = False

class UsageStats(BaseModel):
    requests_used: int
    requests_limit: int
    requests_remaining: int
    window_reset: datetime
    tier: str

class MultiFileGenerateRequest(BaseModel):
    description: str = Field(..., min_length=3, max_length=5000, description="Description of the app to generate")
    framework: str = Field(default="react", description="Framework to use (react, vue, angular, nextjs, etc.)")
    styling: str = Field(default="tailwind", description="Styling framework (tailwind, bootstrap, etc.)")
    features: Optional[List[str]] = Field(default=None, description="List of features to include")
    provider: str = Field(default="openai", description="AI provider: 'openai' or 'anthropic'")
    model: Optional[str] = Field(default=None, description="Specific model (defaults to gpt-4o for OpenAI, claude-3-7-sonnet for Anthropic)")

class FileOutput(BaseModel):
    path: str
    content: str
    language: str

class MultiFileGenerateResponse(BaseModel):
    files: List[FileOutput]
    instructions: str
    dependencies: Dict[str, str]

# ============================================
# ENDPOINTS
# ============================================

@app.get("/", response_model=Dict[str, str])
async def root():
    return {
        "service": "VPN Enterprise AI API",
        "status": "running",
        "version": "2.0.0",
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now(),
        "service": "ai-api",
        "version": "2.0.0",
        "environment": os.getenv("ENVIRONMENT", "production")
    }

@app.get("/usage")
async def get_usage(user: Dict[str, Any] = Depends(verify_token)):
    """Get current usage stats"""
    rate_check = await check_rate_limit(
        user.get("user_id", "anonymous"),
        user.get("tier", "free")
    )
    
    return UsageStats(
        requests_used=rate_check["requests_used"],
        requests_limit=rate_check["requests_limit"],
        requests_remaining=rate_check["requests_remaining"],
        window_reset=rate_check["window_reset"],
        tier=user.get("tier", "free")
    )

# Legacy Ollama endpoint removed - Use /ai/generate/app with OpenAI/Anthropic instead

@app.post("/ai/generate/app", response_model=MultiFileGenerateResponse)
async def generate_full_app(
    request: MultiFileGenerateRequest,
    user: Dict[str, Any] = Depends(verify_token),
    http_request: Request = None
):
    """
    Generate a complete application with multiple files - MORE POWERFUL than Cursor/Lovable
    Uses OpenAI GPT-4o or Anthropic Claude 3.7 Sonnet for superior code generation
    """
    # Debug logging
    logger.info(f"ðŸ“¥ Received app generation request:")
    logger.info(f"   Description length: {len(request.description)} chars")
    logger.info(f"   Framework: {request.framework}")
    logger.info(f"   Provider: {request.provider}")
    logger.info(f"   User tier: {user.get('tier', 'unknown')}")
    
    # Rate limiting
    rate_check = await check_rate_limit(
        user.get("user_id", "anonymous"),
        user.get("tier", "free")
    )
    
    if not rate_check["allowed"]:
        raise HTTPException(
            status_code=429,
            detail=f"Rate limit exceeded. Reset at {rate_check['window_reset']}"
        )
    
    # Validate AI provider availability
    provider = request.provider.lower()
    if provider == "openai" and not openai_client:
        raise HTTPException(
            status_code=503,
            detail="OpenAI API key not configured. Set OPENAI_API_KEY environment variable."
        )
    elif provider == "anthropic" and not anthropic_client:
        raise HTTPException(
            status_code=503,
            detail="Anthropic API key not configured. Set ANTHROPIC_API_KEY environment variable."
        )
    
    features_str = "\n".join([f"- {f}" for f in request.features]) if request.features else "- Basic CRUD operations\n- Responsive design\n- Error handling"
    
    # Enhanced prompt for PLATFORM-READY code generation
    system_prompt = """You are a code generation API for VPN Enterprise Platform.
Generate production-ready apps that deploy directly to our platform.
Apps have automatic database provisioning and hosting.
ONLY return valid JSON - no markdown, no explanations.
Start response with { and end with }."""

    user_prompt = f"""Generate a complete {request.framework} app: {request.description}

Features: {features_str}
Styling: {request.styling}
Platform: VPN Enterprise (auto-provisioned Postgres database, Node.js hosting)

IMPORTANT - Use platform services:
- Database: process.env.DATABASE_URL (auto-provided Postgres connection)
- API endpoint: process.env.PLATFORM_API_URL
- Authentication: Platform JWT tokens in headers

Return ONLY this JSON structure:
{{
  "files": [
    {{"path": "src/App.tsx", "content": "import React from 'react';\\n\\nfunction App() {{\\n  return <div>My App</div>;\\n}}\\n\\nexport default App;", "language": "typescript"}},
    {{"path": "package.json", "content": "{{\\"name\\":\\"app\\",\\"scripts\\":{{\\"dev\\":\\"vite\\",\\"build\\":\\"vite build\\"}}}}", "language": "json"}},
    {{"path": ".env.example", "content": "DATABASE_URL=\\nPLATFORM_API_URL=\\nNODE_ENV=production", "language": "text"}}
  ],
  "instructions": "1. Install: npm install\\n2. Deploy: Click 'Deploy to Platform'\\n3. Database and hosting provisioned automatically",
  "dependencies": {{"react": "^18.3.0", "vite": "^5.0.0"}},
  "requires_database": true
}}

Generate 10-15 files. Response MUST be pure JSON starting with {{ and ending with }}."""

    try:
        # Call appropriate AI provider
        if provider == "openai":
            model = request.model or "gpt-4o"  # GPT-4o is excellent for code generation
            logger.info(f"ðŸ¤– Generating app with OpenAI {model}...")
            
            response = await openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                max_tokens=16000,  # GPT-4o can handle large outputs
                response_format={"type": "json_object"}  # Force JSON output
            )
            
            ai_response = response.choices[0].message.content
            
        elif provider == "anthropic":
            model = request.model or "claude-3-haiku-20240307"  # Claude 3 Haiku - Fast and affordable
            logger.info(f"ðŸ¤– Generating app with Anthropic {model}...")
            
            response = await anthropic_client.messages.create(
                model=model,
                max_tokens=4096,  # Claude 3 Haiku max
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7
            )
            
            ai_response = response.content[0].text
            
        else:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid provider: {provider}. Use 'openai' or 'anthropic'"
            )
        
        # Parse JSON from AI response
        try:
            # Clean up response - remove markdown code blocks if present
            if "```json" in ai_response:
                json_start = ai_response.find("```json") + 7
                json_end = ai_response.find("```", json_start)
                ai_response = ai_response[json_start:json_end].strip()
            elif "```" in ai_response:
                json_start = ai_response.find("```") + 3
                json_end = ai_response.find("```", json_start)
                ai_response = ai_response[json_start:json_end].strip()
            
            result = json.loads(ai_response)
            
            logger.info(f"âœ… Successfully generated {len(result.get('files', []))} files")
            
            return MultiFileGenerateResponse(
                files=[FileOutput(**f) for f in result.get("files", [])],
                instructions=result.get("instructions", "No instructions provided"),
                dependencies=result.get("dependencies", {})
            )
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse AI response as JSON: {e}")
            logger.error(f"Raw response: {ai_response[:500]}...")
            
            # Fallback: create a single file with the response
            return MultiFileGenerateResponse(
                files=[FileOutput(
                    path="App.tsx",
                    content=ai_response,
                    language="typescript"
                )],
                instructions="AI response could not be parsed. Raw output provided.",
                dependencies={}
            )
            
    except Exception as e:
        logger.error(f"AI request failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"AI service error: {str(e)}"
        )

# ============================================
# APP DEPLOYMENT TO PLATFORM
# ============================================

class DeployAppRequest(BaseModel):
    app_name: str = Field(..., min_length=1, max_length=50)
    files: List[FileOutput]
    dependencies: Dict[str, str]
    framework: str = Field(default="react")
    requires_database: bool = Field(default=True)
    user_id: str  # From auth token

class DeploymentResponse(BaseModel):
    deployment_id: str
    app_name: str
    status: str
    database: Optional[Dict[str, Any]] = None
    hosting: Optional[Dict[str, Any]] = None
    app_url: Optional[str] = None
    environment: Optional[Dict[str, str]] = None
    steps: List[Dict[str, str]]

@app.post("/ai/deploy/app", response_model=DeploymentResponse)
async def deploy_generated_app(request: DeployAppRequest):
    """
    Deploy an AI-generated app to the VPN Enterprise platform
    - Creates tenant database (Postgres)
    - Provisions hosting service
    - Deploys files and dependencies
    - Configures environment
    - Returns live app URL
    """
    
    logger.info(f"ðŸš€ Deploying app: {request.app_name} for user: {request.user_id}")
    
    try:
        # Deploy the app
        result = await deployment_service.deploy_app(
            user_id=request.user_id,
            app_name=request.app_name,
            files=[f.dict() for f in request.files],
            dependencies=request.dependencies,
            framework=request.framework,
            requires_database=request.requires_database
        )
        
        return DeploymentResponse(**result)
        
    except Exception as e:
        logger.error(f"Deployment failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Deployment failed: {str(e)}"
        )

# Legacy Ollama endpoints removed - Use OpenAI/Anthropic via /ai/generate/app
# SQL assistance and model listing deprecated

# ============================================
# AUTHENTICATION & KEY MANAGEMENT
# ============================================

class CreateKeyRequest(BaseModel):
    tenant_id: str = Field(..., min_length=1)
    tier: str = Field(default="free", pattern="^(free|pro|enterprise|unlimited)$")
    description: Optional[str] = None
    expires_in_days: Optional[int] = Field(default=365, ge=1, le=3650)

class APIKeyResponse(BaseModel):
    api_key: str
    key_id: str
    tier: str
    rate_limit: Dict[str, int]
    expires_at: Optional[datetime]
    created_at: datetime

@app.post("/auth/create-key", response_model=APIKeyResponse)
async def create_api_key(request: CreateKeyRequest):
    """
    Generate a new API key for a tenant
    NOTE: The full API key is only shown once. Store it securely.
    """
    import secrets
    import uuid
    
    # Generate secure API key
    key_prefix = "vpn"
    key_secret = secrets.token_urlsafe(32)
    api_key = f"{key_prefix}_{key_secret}"
    
    # Generate key ID
    key_id = str(uuid.uuid4())
    
    # Hash the key for storage (SHA256)
    key_hash = hashlib.sha256(api_key.encode()).hexdigest()
    
    # Calculate expiration
    expires_at = datetime.now() + timedelta(days=request.expires_in_days)
    
    # Get rate limit for tier
    rate_limit = RATE_LIMITS.get(request.tier, RATE_LIMITS["free"])
    
    # TODO: Store in database (ai_api_keys table)
    # For now, we'll return the key. In production, this should:
    # 1. Insert into ai_api_keys table with key_hash
    # 2. Associate with tenant_id
    # 3. Log the creation event
    
    logger.info(f"API key created for tenant {request.tenant_id} with tier {request.tier}")
    
    return APIKeyResponse(
        api_key=api_key,  # Only shown once!
        key_id=key_id,
        tier=request.tier,
        rate_limit=rate_limit,
        expires_at=expires_at,
        created_at=datetime.now()
    )

@app.post("/auth/verify-key")
async def verify_api_key(authorization: str = Header(...)):
    """Verify an API key and return its details"""
    if not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Invalid authorization header")
    
    api_key = authorization.split(" ")[1]
    
    # Hash the provided key
    key_hash = hashlib.sha256(api_key.encode()).hexdigest()
    
    # TODO: Look up in database by key_hash
    # For now, accept any key with vpn_ prefix
    if not api_key.startswith("vpn_"):
        raise HTTPException(status_code=401, detail="Invalid API key")
    
    return {
        "valid": True,
        "tier": "free",  # Would come from database
        "tenant_id": "demo-tenant"
    }

# ============================================
# ADMIN ENDPOINTS
# ============================================

@app.post("/admin/cache/clear")
async def clear_cache(user: Dict[str, Any] = Depends(verify_token)):
    """Clear all caches (admin only)"""
    if user.get("tier") != "enterprise":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    memory_cache.clear()
    rate_limit_store.clear()
    return {"message": "Cache cleared successfully"}

@app.get("/admin/stats")
async def get_stats(user: Dict[str, Any] = Depends(verify_token)):
    """Get service statistics (admin only)"""
    if user.get("tier") != "enterprise":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    return {
        "cache_size": len(memory_cache),
        "rate_limit_users": len(rate_limit_store),
        "total_requests": sum(len(v) for v in rate_limit_store.values())
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app_production:app",
        host="0.0.0.0",
        port=5001,
        reload=False,
        log_level="info",
        workers=4  # Multiple workers for production
    )
