"""
VPN Enterprise - Production AI API (Enterprise Grade)
=========================================================
World-class AI code generation powered by OpenAI GPT-4o & Anthropic Claude 3.5 Sonnet
Integrated with N8N for automated deployment, testing, and monitoring

Features:
- Dual AI Provider Support (OpenAI + Anthropic) with intelligent routing
- N8N Webhook Integration for CI/CD automation
- Enterprise-grade rate limiting, caching, and monitoring
- Automated database provisioning for generated apps
- Security scanning and code quality checks
- Real-time deployment status tracking

Author: VPN Enterprise Team
Version: 2.0.0 (Production)
"""

from fastapi import FastAPI, HTTPException, status, Request, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, ValidationError
from datetime import datetime, timedelta
from contextlib import asynccontextmanager
from typing import Optional, Dict, Any, List, Literal
import httpx
import os
import logging
import hashlib
import json
import time
import asyncio
import re
from enum import Enum

# AI Provider imports
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============================================
# HELPER FUNCTIONS
# ============================================

def read_secret(env_var_name: str, file_env_var_name: str) -> str:
    """
    Read secret from file if *_FILE env var is set, otherwise from direct env var.
    This follows Docker secrets pattern used throughout the application.
    """
    # First check if there's a *_FILE env var pointing to a secret file
    secret_file_path = os.getenv(file_env_var_name, "")
    if secret_file_path and os.path.exists(secret_file_path):
        try:
            with open(secret_file_path, 'r') as f:
                secret = f.read().strip()
                if secret:
                    logger.info(f"‚úÖ Loaded {env_var_name} from secret file: {secret_file_path}")
                    return secret
        except Exception as e:
            logger.error(f"‚ùå Failed to read {secret_file_path}: {e}")
    
    # Fallback to direct env var
    direct_value = os.getenv(env_var_name, "")
    if direct_value:
        logger.info(f"‚úÖ Loaded {env_var_name} from environment variable")
    return direct_value

def sanitize_and_parse_json(text: str, context: str = "AI response") -> dict:
    """
    Sanitize AI-generated JSON and parse it safely.
    Handles common issues: markdown fences, control characters, malformed strings.
    
    Args:
        text: Raw text response from AI
        context: Description for error messages
    
    Returns:
        Parsed JSON dict
    
    Raises:
        ValueError: If JSON cannot be parsed even after sanitization
    """
    try:
        # 1. Remove markdown code fences
        cleaned = text.strip()
        if cleaned.startswith("```"):
            cleaned = re.sub(r'```json?\n?', '', cleaned)
            cleaned = re.sub(r'\n?```$', '', cleaned)
        
        # 2. Try parsing first (fast path for clean JSON)
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            pass
        
        # 3. Fix common control character issues in file content
        # Replace literal control characters with escaped versions in strings
        # This handles cases where AI includes raw newlines in JSON strings
        cleaned = re.sub(r'(?<!\\)\n(?=.*")', '\\n', cleaned)  # Unescaped newlines
        cleaned = re.sub(r'(?<!\\)\t', '\\t', cleaned)  # Unescaped tabs
        cleaned = re.sub(r'(?<!\\)\r', '\\r', cleaned)  # Unescaped carriage returns
        
        # Try parsing again
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError as e:
            # Log the problematic area
            error_pos = e.pos if hasattr(e, 'pos') else 0
            snippet_start = max(0, error_pos - 100)
            snippet_end = min(len(cleaned), error_pos + 100)
            problematic = cleaned[snippet_start:snippet_end]
            logger.warning(f"JSON parse failed in {context} near position {error_pos}: {problematic[:200]}")
            raise ValueError(f"AI returned invalid JSON in {context}: {str(e)}")
            
    except Exception as e:
        logger.error(f"Failed to sanitize and parse JSON from {context}: {str(e)}")
        raise ValueError(f"AI returned unparseable JSON in {context}: {str(e)}")

# ============================================
# CONFIGURATION
# ============================================

# AI Provider Keys (support both direct env vars and Docker secrets)
OPENAI_API_KEY = read_secret("OPENAI_API_KEY", "OPENAI_API_KEY_FILE")
ANTHROPIC_API_KEY = read_secret("ANTHROPIC_API_KEY", "ANTHROPIC_API_KEY_FILE")

# N8N Webhook Configuration
N8N_WEBHOOK_BASE = os.getenv("N8N_WEBHOOK_URL", "https://chatbuilds.com/webhook")
N8N_APP_GENERATED = f"{N8N_WEBHOOK_BASE}/nexusai-app-generated"
N8N_APP_DEPLOY = f"{N8N_WEBHOOK_BASE}/nexusai-deploy"
N8N_APP_ERROR = f"{N8N_WEBHOOK_BASE}/nexusai-error"

# Service URLs
API_BASE_URL = os.getenv("API_URL", "https://chatbuilds.com/api")
DATABASE_PROVISIONER_URL = os.getenv("DATABASE_PROVISIONER_URL", "http://localhost:3003")

# Cache & Rate Limiting
CACHE_TTL = 3600  # 1 hour
memory_cache: Dict[str, Any] = {}
rate_limit_store: Dict[str, List[float]] = {}

# Rate limits by subscription tier
RATE_LIMITS = {
    "free": {"ai": 10, "deploy": 5, "window": 3600},      # 10 AI calls/hour, 5 deploys/day
    "pro": {"ai": 100, "deploy": 50, "window": 3600},     # 100 AI calls/hour, 50 deploys/day
    "enterprise": {"ai": 1000, "deploy": 500, "window": 3600}  # Unlimited
}

# AI Model Configuration
OPENAI_MODELS = {
    "gpt-4o": {"cost": 10, "max_tokens": 16384, "best_for": "complex apps"},
    "gpt-4o-mini": {"cost": 5, "max_tokens": 16384, "best_for": "simple apps"},
    "gpt-3.5-turbo": {"cost": 3, "max_tokens": 4096, "best_for": "quick prototypes"}
}

ANTHROPIC_MODELS = {
    "claude-3-5-sonnet-20241022": {"cost": 10, "max_tokens": 8192, "best_for": "balanced"},
    "claude-3-5-sonnet-20240620": {"cost": 10, "max_tokens": 8192, "best_for": "legacy"},
    "claude-3-opus-20240229": {"cost": 12, "max_tokens": 8192, "best_for": "production code"},
    "claude-3-haiku-20240307": {"cost": 5, "max_tokens": 4096, "best_for": "fast generation"}
}

# Initialize clients
openai_client = None
anthropic_client = None
http_client: Optional[httpx.AsyncClient] = None

if OPENAI_API_KEY:
    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    logger.info("‚úÖ OpenAI client initialized with GPT-4o access")
    
if ANTHROPIC_API_KEY:
    anthropic_client = AsyncAnthropic(api_key=ANTHROPIC_API_KEY)
    logger.info("‚úÖ Anthropic client initialized with Claude 3.5 Sonnet access")

# ============================================
# LIFESPAN & APP INITIALIZATION
# ============================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan - startup and shutdown"""
    global http_client
    
    logger.info("=" * 60)
    logger.info("üöÄ VPN ENTERPRISE AI API - PRODUCTION MODE")
    logger.info("=" * 60)
    logger.info(f"üìÖ Started: {datetime.utcnow().isoformat()}")
    logger.info(f"ü§ñ AI Providers Active:")
    if openai_client:
        logger.info("   ‚úÖ OpenAI GPT-4o (Industry Leading)")
    if anthropic_client:
        logger.info("   ‚úÖ Anthropic Claude 3.5 Sonnet (Code Specialist)")
    
    logger.info(f"üîó N8N Webhooks:")
    logger.info(f"   ‚Ä¢ App Generated: {N8N_APP_GENERATED}")
    logger.info(f"   ‚Ä¢ App Deploy: {N8N_APP_DEPLOY}")
    logger.info(f"   ‚Ä¢ App Error: {N8N_APP_ERROR}")
    
    if not openai_client and not anthropic_client:
        # Don't crash the container in production. Keep the service up so
        # Nginx health checks stay green and callers get a clear 503.
        logger.error("‚ùå NO AI PROVIDERS! Set OPENAI_API_KEY or ANTHROPIC_API_KEY")
    
    # Initialize HTTP client for webhooks
    http_client = httpx.AsyncClient(timeout=10.0)
    logger.info("‚úÖ HTTP client initialized for N8N webhooks")
    logger.info("=" * 60)
    
    yield
    
    # Shutdown
    if http_client:
        await http_client.aclose()
    logger.info("üõë API shutting down gracefully")

app = FastAPI(
    title="VPN Enterprise AI API",
    description="Enterprise-grade AI code generation with OpenAI & Anthropic",
    version="2.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# MODELS & SCHEMAS
# ============================================

class AIProvider(str, Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    AUTO = "auto"  # Intelligent routing

class AppFramework(str, Enum):
    REACT = "react"
    VUE = "vue"
    NEXTJS = "nextjs"
    EXPRESS = "express"
    FASTAPI = "fastapi"
    FLASK = "flask"

class StylingFramework(str, Enum):
    TAILWIND = "tailwind"
    CSS = "css"
    STYLED_COMPONENTS = "styled-components"
    MUI = "mui"

class AIGenerateRequest(BaseModel):
    prompt: str = Field(..., min_length=10, max_length=100000)
    model: Optional[str] = Field(default=None)
    provider: AIProvider = Field(default=AIProvider.AUTO)
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    max_tokens: int = Field(default=8192, le=16384)

class MultiFileAppRequest(BaseModel):
    description: str = Field(..., min_length=20, max_length=10000)
    framework: AppFramework = Field(default=AppFramework.REACT)
    styling: StylingFramework = Field(default=StylingFramework.TAILWIND)
    features: List[str] = Field(default_factory=list)
    provider: AIProvider = Field(default=AIProvider.AUTO)
    include_database: bool = Field(default=True)
    include_auth: bool = Field(default=False)
    include_api: bool = Field(default=True)

class FileOutput(BaseModel):
    path: str
    content: str
    language: str

class MultiFileAppResponse(BaseModel):
    files: List[FileOutput]
    instructions: str
    dependencies: Dict[str, str]
    requires_database: bool
    database_schema: Optional[str] = None
    database_info: Optional[Dict[str, Any]] = None  # Database connection credentials and info
    app_id: Optional[str] = None  # Saved app ID if persisted
    deployment_config: Optional[Dict[str, Any]] = None
    provider_used: str
    generation_time_ms: int
    tokens_used: int

class DeploymentRequest(BaseModel):
    model_config = {"extra": "allow"}

    app_name: str
    files: List[FileOutput]
    dependencies: Dict[str, str] = Field(default_factory=dict)
    framework: str = Field(default="react")
    requires_database: bool
    database_schema: Optional[str] = None
    user_id: str = Field(default="anonymous")
    user_email: Optional[str] = None
    app_id: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    timestamp: datetime
    version: str
    ai_providers: Dict[str, bool]
    n8n_enabled: bool

class ModelsResponse(BaseModel):
    models: List[str]

class SQLAssistRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=100000)
    action: str = Field(..., pattern="^(generate|explain|optimize|fix)$")
    schema: Optional[str] = None
    sql: Optional[str] = None
    provider: AIProvider = Field(default=AIProvider.AUTO)
    model: Optional[str] = None

class SQLAssistResponse(BaseModel):
    sql: Optional[str] = None
    explanation: Optional[str] = None
    optimized: Optional[str] = None
    fixed: Optional[str] = None
    suggestions: Optional[List[str]] = None
    provider_used: Optional[str] = None
    model_used: Optional[str] = None

class AIProvidersResponse(BaseModel):
    providers: List[Dict[str, str]]
    routing_strategy: str

class DeployAppRequest(BaseModel):
    app_name: str = Field(..., min_length=1, max_length=80)
    files: List[FileOutput]
    dependencies: Dict[str, str] = Field(default_factory=dict)
    framework: str = Field(default="react")
    requires_database: bool = Field(default=True)
    user_id: str = Field(default="anonymous")
    # Optional fields used by other flows
    app_id: Optional[str] = None
    database_schema: Optional[str] = None
    user_email: Optional[str] = None

class DeploymentStatusResponse(BaseModel):
    deployment_id: str
    status: str
    progress: int
    current_step: str
    logs: List[str]
    error: Optional[str] = None

class DeploymentResponse(BaseModel):
    deployment_id: str
    app_name: str
    status: str
    database: Optional[Dict[str, Any]] = None
    hosting: Optional[Dict[str, Any]] = None
    app_url: Optional[str] = None
    environment: Optional[Dict[str, str]] = None
    steps: List[Dict[str, str]]

# ============================================
# UTILITY FUNCTIONS
# ============================================

def cache_key(prefix: str, *args: Any) -> str:
    """Generate cache key"""
    data = json.dumps(args, sort_keys=True)
    hash_val = hashlib.md5(data.encode()).hexdigest()
    return f"{prefix}:{hash_val}"

def get_cache(key: str) -> Optional[Any]:
    """Get from cache"""
    if key in memory_cache:
        entry = memory_cache[key]
        if entry["expires"] > time.time():
            logger.info(f"üíæ Cache HIT: {key}")
            return entry["data"]
        else:
            del memory_cache[key]
    return None

def set_cache(key: str, data: Any, ttl: int = CACHE_TTL):
    """Set cache"""
    memory_cache[key] = {
        "data": data,
        "expires": time.time() + ttl
    }
    logger.info(f"üíæ Cache SET: {key} (TTL: {ttl}s)")

def check_rate_limit(user_id: str, tier: str, limit_type: str = "ai") -> bool:
    """Check if user is within rate limits"""
    if tier not in RATE_LIMITS:
        tier = "free"
    
    limits = RATE_LIMITS[tier]
    limit = limits.get(limit_type, limits["ai"])
    window = limits["window"]
    
    key = f"{user_id}:{limit_type}"
    now = time.time()
    
    if key not in rate_limit_store:
        rate_limit_store[key] = []
    
    # Remove old entries
    rate_limit_store[key] = [t for t in rate_limit_store[key] if now - t < window]
    
    if len(rate_limit_store[key]) >= limit:
        logger.warning(f"‚ö†Ô∏è  Rate limit exceeded for {user_id} ({tier}): {len(rate_limit_store[key])}/{limit}")
        return False
    
    rate_limit_store[key].append(now)
    return True

async def send_n8n_webhook(webhook_url: str, payload: Dict[str, Any]):
    """Send webhook to N8N (fire and forget)"""
    try:
        if http_client:
            await http_client.post(webhook_url, json=payload, timeout=5.0)
            logger.info(f"üîî N8N webhook sent: {webhook_url}")
    except Exception as e:
        logger.error(f"‚ùå N8N webhook failed: {str(e)}")

def choose_ai_provider(description: str, provider: AIProvider) -> tuple[str, Any]:
    """Intelligently choose AI provider"""
    if provider == AIProvider.OPENAI and openai_client:
        return ("openai", openai_client)
    
    if provider == AIProvider.ANTHROPIC and anthropic_client:
        return ("anthropic", anthropic_client)
    
    # AUTO mode - intelligent routing
    if provider == AIProvider.AUTO:
        # Prefer OpenAI GPT-4o as primary (most reliable and widely available)
        if openai_client:
            logger.info("üß† AUTO: Chose GPT-4o (primary provider)")
            return ("openai", openai_client)
        
        # Fallback to Claude if OpenAI unavailable
        if anthropic_client:
            logger.info("üß† AUTO: Chose Claude 3.5 Sonnet (fallback)")
            return ("anthropic", anthropic_client)
    
    raise HTTPException(
        status_code=503,
        detail="No AI provider available for the requested provider type"
    )

def _default_openai_model() -> str:
    return "gpt-4o"

def _default_anthropic_model() -> str:
    # Use Claude 3 Haiku - fast, reliable, widely available
    # Keep this aligned with ANTHROPIC_MODELS.
    return "claude-3-haiku-20240307"

def _get_model_max_tokens(provider_name: str, model: str) -> int:
    """Get the appropriate max_tokens for a model."""
    if provider_name == "anthropic":
        model_info = ANTHROPIC_MODELS.get(model, {})
        return model_info.get("max_tokens", 4096)  # Default to 4096 for safety
    else:  # openai
        model_info = OPENAI_MODELS.get(model, {})
        return model_info.get("max_tokens", 16000)

def _normalize_requested_model(provider_name: str, requested_model: Optional[str]) -> str:
    """Avoid passing invalid/placeholder model names upstream.

    The NexusAI frontend historically used non-OpenAI/Anthropic defaults (e.g. Ollama
    names like deepseek-coder-v2). Those should safely fall back to a valid model.
    """
    if not requested_model:
        return _default_openai_model() if provider_name == "openai" else _default_anthropic_model()

    requested_model = requested_model.strip()
    if requested_model in {"auto", "default"}:
        return _default_openai_model() if provider_name == "openai" else _default_anthropic_model()

    if provider_name == "openai":
        if requested_model in OPENAI_MODELS:
            return requested_model
        # Allow direct OpenAI model names even if not listed.
        if requested_model.startswith("gpt-"):
            return requested_model
        return _default_openai_model()

    # anthropic
    if requested_model in ANTHROPIC_MODELS:
        return requested_model
    if requested_model.startswith("claude-"):
        return requested_model
    return _default_anthropic_model()

_deployment_status_store: Dict[str, DeploymentStatusResponse] = {}

# ============================================
# PROMPT TEMPLATES
# ============================================

def get_app_generation_prompt(request: MultiFileAppRequest) -> str:
    """Generate comprehensive prompt for app generation"""
    
    features_text = ", ".join(request.features) if request.features else "basic CRUD operations"
    
    prompt = f"""Generate a complete, production-ready {request.framework.value} application with the following requirements:

**Project Description:**
{request.description}

**Technical Stack:**
- Framework: {request.framework.value}
- Styling: {request.styling.value}
- Features: {features_text}
- Database: {"PostgreSQL (with schema)" if request.include_database else "None"}
- Authentication: {"Yes (JWT-based)" if request.include_auth else "No"}
- API Layer: {"Yes (REST API)" if request.include_api else "No"}

**Requirements:**
1. Generate COMPLETE source files (no placeholders or TODO comments)
2. Include proper error handling and validation
3. Use modern best practices and patterns
4. Add TypeScript types where applicable
5. Include package.json with all dependencies
6. Add README with setup instructions
7. Include environment variable template (.env.example)
8. Add Dockerfile for containerization
9. {"Generate PostgreSQL database schema with tables, indexes, and constraints" if request.include_database else ""}

**Output Format:**
Return a JSON object with this EXACT structure:
{{
    "files": [
        {{"path": "src/App.tsx", "content": "...", "language": "typescript"}},
        {{"path": "package.json", "content": "...", "language": "json"}},
        ...
    ],
    "instructions": "Step-by-step setup instructions",
    "dependencies": {{"react": "^18.0.0", ...}},
    "requires_database": true/false,
    "database_schema": "CREATE TABLE statements..." (if database is needed)
}}

**CRITICAL:** Return ONLY the JSON object, no markdown, no explanations, just pure JSON.
"""
    
    return prompt

# ============================================
# API ENDPOINTS
# ============================================

@app.get("/", response_model=Dict[str, Any])
async def root():
    """Root endpoint"""
    return {
        "service": "VPN Enterprise AI API",
        "version": "2.0.0",
        "status": "operational",
        "ai_providers": {
            "openai": openai_client is not None,
            "anthropic": anthropic_client is not None
        },
        "endpoints": {
            "health": "/health",
            "docs": "/docs",
            "generate": "/ai/generate",
            "generate_app": "/ai/generate/app",
            "deploy": "/ai/deploy/app"
        }
    }

@app.get("/health", response_model=HealthResponse)
@app.get("/ai/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    any_provider = (openai_client is not None) or (anthropic_client is not None)
    return HealthResponse(
        status="healthy" if any_provider else "degraded",
        timestamp=datetime.utcnow(),
        version="2.0.0",
        ai_providers={
            "openai": openai_client is not None,
            "anthropic": anthropic_client is not None
        },
        n8n_enabled=bool(N8N_WEBHOOK_BASE)
    )

@app.get("/models", response_model=ModelsResponse)
@app.get("/ai/models", response_model=ModelsResponse)
async def list_models():
    """List models supported by configured providers."""
    models: List[str] = []
    if openai_client is not None:
        models.extend(list(OPENAI_MODELS.keys()))
    if anthropic_client is not None:
        models.extend(list(ANTHROPIC_MODELS.keys()))

    # If no providers configured, still return a stable response.
    if not models:
        models = list(OPENAI_MODELS.keys()) + list(ANTHROPIC_MODELS.keys())

    # Deduplicate while keeping order.
    seen = set()
    unique_models: List[str] = []
    for m in models:
        if m in seen:
            continue
        unique_models.append(m)
        seen.add(m)

    return ModelsResponse(models=unique_models)

@app.get("/ai/providers", response_model=AIProvidersResponse)
@app.get("/ai/ai/providers", response_model=AIProvidersResponse)
async def ai_providers():
    """Expose provider availability for the frontend UI."""
    providers: List[Dict[str, str]] = []
    providers.append({
        "name": "OpenAI",
        "model": _default_openai_model(),
        "status": "available" if openai_client is not None else "unconfigured",
    })
    providers.append({
        "name": "Anthropic",
        "model": _default_anthropic_model(),
        "status": "available" if anthropic_client is not None else "unconfigured",
    })
    return AIProvidersResponse(providers=providers, routing_strategy="auto")

@app.post("/sql/assist", response_model=SQLAssistResponse)
@app.post("/ai/sql/assist", response_model=SQLAssistResponse)
async def sql_assist(request: SQLAssistRequest):
    """SQL assistance endpoint used by the NexusAI frontend."""
    provider_name, client = choose_ai_provider(request.query, request.provider)
    model = _normalize_requested_model(provider_name, request.model)

    schema_text = f"\n\nSchema:\n{request.schema}" if request.schema else ""
    sql_text = f"\n\nSQL:\n{request.sql}" if request.sql else ""

    if request.action == "generate":
        prompt = f"Generate PostgreSQL SQL for this request:\n{request.query}{schema_text}\nReturn ONLY SQL."
    elif request.action == "explain":
        prompt = f"Explain this SQL in plain English, including performance considerations.\n{request.query}{schema_text}{sql_text}"
    elif request.action == "optimize":
        prompt = f"Optimize this SQL for PostgreSQL. Provide optimized SQL only.\n{request.query}{schema_text}{sql_text}"
    else:  # fix
        prompt = f"Fix this SQL for PostgreSQL. Provide fixed SQL only.\n{request.query}{schema_text}{sql_text}"

    try:
        if provider_name == "openai":
            response = await client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=2048,
            )
            text = response.choices[0].message.content
        else:
            response = await client.messages.create(
                model=model,
                max_tokens=2048,
                temperature=0.2,
                messages=[{"role": "user", "content": prompt}],
            )
            text = response.content[0].text

        text = (text or "").strip()
        if request.action == "explain":
            return SQLAssistResponse(
                explanation=text,
                provider_used=provider_name,
                model_used=model,
            )
        if request.action == "optimize":
            return SQLAssistResponse(
                optimized=text,
                provider_used=provider_name,
                model_used=model,
            )
        if request.action == "fix":
            return SQLAssistResponse(
                fixed=text,
                provider_used=provider_name,
                model_used=model,
            )
        return SQLAssistResponse(sql=text, provider_used=provider_name, model_used=model)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå SQL assist failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"SQL assist failed: {str(e)}")

@app.post("/ai/generate", response_model=Dict[str, Any])
async def generate_ai_text(
    request: AIGenerateRequest,
    x_api_key: Optional[str] = Header(None)
):
    """Generate AI text/code"""
    start_time = time.time()
    
    # Choose provider
    provider_name, client = choose_ai_provider(request.prompt, request.provider)
    
    try:
        if provider_name == "openai":
            model = _normalize_requested_model(provider_name, request.model)
            response = await client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": request.prompt}],
                temperature=request.temperature,
                max_tokens=request.max_tokens
            )
            result = response.choices[0].message.content
            tokens = response.usage.total_tokens if response.usage else 0
            
        else:  # anthropic
            model = _normalize_requested_model(provider_name, request.model)
            response = await client.messages.create(
                model=model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                messages=[{"role": "user", "content": request.prompt}]
            )
            result = response.content[0].text
            tokens = response.usage.input_tokens + response.usage.output_tokens if hasattr(response, 'usage') else 0
        
        elapsed = int((time.time() - start_time) * 1000)
        
        return {
            "response": result,
            "provider": provider_name,
            "model": model,
            "tokens_used": tokens,
            "generation_time_ms": elapsed
        }
        
    except Exception as e:
        logger.error(f"‚ùå AI generation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AI generation failed: {str(e)}")

@app.post("/ai/generate/app", response_model=MultiFileAppResponse)
async def generate_full_app(
    request: MultiFileAppRequest,
    x_api_key: Optional[str] = Header(None),
    x_user_id: Optional[str] = Header(None),
    x_user_tier: Optional[str] = Header("free")
):
    """
    Generate complete application with multiple files
    This is the MAIN endpoint for NexusAI
    """
    start_time = time.time()
    user_id = x_user_id or "anonymous"
    
    # Rate limiting
    if not check_rate_limit(user_id, x_user_tier, "ai"):
        raise HTTPException(
            status_code=429,
            detail=f"Rate limit exceeded for {x_user_tier} tier"
        )
    
    logger.info(f"üé® Generating {request.framework.value} app for user {user_id}")
    logger.info(f"üìù Description: {request.description[:100]}...")
    
    # Check cache
    cache_key_val = cache_key("app", request.description, request.framework, request.features)
    cached = get_cache(cache_key_val)
    if cached:
        logger.info("üíæ Returning cached app generation")
        return MultiFileAppResponse(**cached)
    
    # Choose AI provider
    provider_name, client = choose_ai_provider(request.description, request.provider)
    
    # Generate prompt
    prompt = get_app_generation_prompt(request)
    
    try:
        # Call AI
        if provider_name == "openai":
            model = _default_openai_model()  # Best for code generation
            response = await client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are an expert full-stack developer. Generate complete, production-ready code with no placeholders. Return valid JSON only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=16000,
                response_format={"type": "json_object"}
            )
            result_text = response.choices[0].message.content
            tokens = response.usage.total_tokens if response.usage else 0
            
        else:  # anthropic
            model = _default_anthropic_model()
            max_tokens = _get_model_max_tokens("anthropic", model)
            response = await client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=0.7,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "You are an expert full-stack developer. Generate complete, production-ready code with no placeholders. Return valid JSON only."},
                            {"type": "text", "text": prompt}
                        ]
                    }
                ]
            )
            result_text = response.content[0].text
            tokens = response.usage.input_tokens + response.usage.output_tokens
        
        # Parse AI response with sanitization
        result_json = sanitize_and_parse_json(result_text, "generate_app")
        
        elapsed = int((time.time() - start_time) * 1000)
        
        # Build response
        response_data = { "files": result_json.get("files", []),
            "instructions": result_json.get("instructions", "No instructions provided"),
            "dependencies": result_json.get("dependencies", {}),
            "requires_database": result_json.get("requires_database", request.include_database),
            "database_schema": result_json.get("database_schema"),
            "deployment_config": {
                "framework": request.framework.value,
                "port": 3000,
                "build_command": "npm run build",
                "start_command": "npm start"
            },
            "provider_used": f"{provider_name}/{model}",
            "generation_time_ms": elapsed,
            "tokens_used": tokens
        }
        
        # Cache result
        set_cache(cache_key_val, response_data, ttl=3600)
        
        # Send N8N webhook (async, non-blocking)
        asyncio.create_task(send_n8n_webhook(N8N_APP_GENERATED, {
            "event": "app_generated",
            "user_id": user_id,
            "app_id": cache_key_val,
            "framework": request.framework.value,
            "provider": provider_name,
            "files_count": len(response_data["files"]),
            "requires_database": response_data["requires_database"],
            "generated_at": datetime.utcnow().isoformat()
        }))
        
        logger.info(f"‚úÖ App generated successfully: {len(response_data['files'])} files in {elapsed}ms using {provider_name}")
        
        return MultiFileAppResponse(**response_data)
        
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå Failed to parse AI response as JSON: {str(e)}")
        logger.error(f"Raw response: {result_text[:500]}...")
        raise HTTPException(
            status_code=500,
            detail="AI returned invalid JSON. Please try again."
        )
    except Exception as e:
        logger.error(f"‚ùå App generation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

@app.post("/deploy/app", response_model=Dict[str, Any])
async def deploy_app(
    request: DeploymentRequest,
    x_api_key: Optional[str] = Header(None),
    x_user_tier: Optional[str] = Header("free")
):
    """
    Deploy generated app
    Triggers N8N workflow for automated deployment
    """
    user_id = request.user_id
    
    # Rate limiting
    if not check_rate_limit(user_id, x_user_tier, "deploy"):
        raise HTTPException(
            status_code=429,
            detail=f"Deploy rate limit exceeded for {x_user_tier} tier"
        )
    
    import uuid

    deployment_id = str(uuid.uuid4())
    logger.info(f"üöÄ Deploying app '{request.app_name}' for user {user_id} (deployment_id={deployment_id})")
    
    # Track a minimal status for frontend polling (best-effort; per-worker memory).
    _deployment_status_store[deployment_id] = DeploymentStatusResponse(
        deployment_id=deployment_id,
        status="pending",
        progress=5,
        current_step="queued",
        logs=["Deployment queued"],
        error=None,
    )

    # Send to N8N for deployment automation
    deployment_payload = {
        "event": "deploy_app",
        "deployment_id": deployment_id,
        "app_id": request.app_id or deployment_id,
        "app_name": request.app_name,
        "user_id": user_id,
        "user_email": getattr(request, "user_email", None),
        "framework": request.framework,
        "files": [{"path": f.path, "content": f.content, "language": f.language} for f in request.files],
        "dependencies": request.dependencies,
        "requires_database": request.requires_database,
        "database_schema": request.database_schema,
        "requested_at": datetime.utcnow().isoformat()
    }
    
    # Send webhook to N8N (this will handle deployment)
    asyncio.create_task(send_n8n_webhook(N8N_APP_DEPLOY, deployment_payload))
    
    return {
        "deployment_id": deployment_id,
        "app_name": request.app_name,
        "status": "pending",
        "steps": [
            {"step": "queued", "status": "success"},
            {"step": "building", "status": "pending"},
            {"step": "deploying", "status": "pending"},
        ],
        "app_url": None,
    }

@app.post("/ai/deploy/app", response_model=Dict[str, Any])
async def deploy_app_ai_prefix(request: DeploymentRequest, x_api_key: Optional[str] = Header(None), x_user_tier: Optional[str] = Header("free")):
    """Alias for deployments when routed through /api/ai/* rewrite."""
    return await deploy_app(request, x_api_key=x_api_key, x_user_tier=x_user_tier)

@app.get("/deploy/status/{deployment_id}", response_model=DeploymentStatusResponse)
@app.get("/ai/deploy/status/{deployment_id}", response_model=DeploymentStatusResponse)
async def get_deployment_status(deployment_id: str):
    """Best-effort deployment status (front-end polling)."""
    status = _deployment_status_store.get(deployment_id)
    if status is None:
        return DeploymentStatusResponse(
            deployment_id=deployment_id,
            status="pending",
            progress=0,
            current_step="queued",
            logs=[],
            error=None,
        )
    return status

# ============================================
# ADVANCED DUAL-AI FULL-STACK GENERATION
# ============================================

@app.post("/ai/generate/fullstack", response_model=MultiFileAppResponse)
async def generate_fullstack_app(
    request: MultiFileAppRequest,
    x_api_key: Optional[str] = Header(None),
    x_user_id: Optional[str] = Header(None)
):
    """
    üöÄ ENTERPRISE-GRADE FULLSTACK GENERATION
    
    **Dual-AI Orchestration System (World's Most Advanced):**
    - Phase 1: Claude 3.5 Sonnet creates comprehensive architecture & database design
    - Phase 2: GPT-4o generates complete production-ready frontend code
    - Phase 3: GPT-4o generates backend API with all endpoints + Postman collection
    - Phase 4: Claude 3.5 Sonnet reviews, integrates, and optimizes everything
    - Phase 5: Automatic database provisioning with tables created in Database-as-a-Service
    
    **More Powerful Than Any Competitor:**
    - ‚úÖ Generates 20-40 complete files (vs Cursor: 5-10, Lovable: 8-12, Bolt: 10-15)
    - ‚úÖ Real backend API with auth, validation, error handling (competitors skip this)
    - ‚úÖ Complete database with indexes, constraints, migrations (competitors use fake data)
    - ‚úÖ Docker + deployment config + CI/CD ready (competitors provide basic setup)
    - ‚úÖ Postman collection for API testing (competitors don't include this)
    - ‚úÖ Production-grade code quality with NO placeholders! (competitors use TODOs)
    - ‚úÖ **AUTOMATIC DATABASE PROVISIONING** - Tables created instantly! (competitors don't do this)
    
    **Result:** Deploy-ready professional application in 30-45 seconds
    """
    start_time = time.time()
    user_id = x_user_id or "anonymous"
    
    # Must have at least OpenAI for generation
    if not openai_client:
        raise HTTPException(
            status_code=503,
            detail="Fullstack generation requires OpenAI API key (GPT-4o)"
        )
    
    # Claude enhances the generation but is not required
    has_anthropic = anthropic_client is not None
    
    try:
        logger.info(f"üéØ Starting DUAL-AI FULLSTACK generation: {request.description[:100]}")
        logger.info(f"ü§ñ Phase 1: Claude architects the system...")
        
        # ==============================================
        # PHASE 1: ARCHITECTURE & DATABASE DESIGN (Claude 3.5 Sonnet)
        # ==============================================
        architecture_prompt = f"""You are a senior software architect. Design a COMPLETE, production-grade full-stack application.

**Project:** {request.description}

**Tech Stack:**
- Frontend: {request.framework.value} + TypeScript + {request.styling.value}
- Backend: Node.js + Express + TypeScript
- Database: PostgreSQL with full schema
- Auth: JWT + bcrypt
- API: RESTful with OpenAPI docs
- Features: {', '.join(request .features) if request.features else 'Full CRUD + Auth + Admin'}

**Architecture Requirements:**
1. **Frontend Structure (15-20 files):**
   - Pages for all major features (Home, Dashboard, Login, Register, Profile, Admin, etc.)
   - Reusable UI components (buttons, forms, modals, cards, tables, navigation)
   - API client with interceptors and error handling
   - Authentication context and protected routes
   - State management (Context API or Redux)
   - Form validation with type safety
   - Responsive layouts and themes

2. **Backend API (10-15 files):**
   - Express server with middleware (cors, helmet, rate-limiting)
   - Authentication endpoints (register, login, refresh, logout)
   - CRUD endpoints for ALL resources
   - Input validation with express-validator
   - Error handling middleware
   - API documentation (Swagger/OpenAPI)
   - Database connection pooling
   - Environment configuration

3. **Database Design (5-10 tables):**
   - Users table (id, email, password_hash, role, created_at, updated_at)
   - Main resource tables with proper relations
   - Junction tables for many-to-many relationships  
   - Indexes on foreign keys and frequently queried columns
   - NOT NULL constraints and defaults
   - ON DELETE CASCADE for dependencies

4. **DevOps & Deployment:**
   - Dockerfile for frontend + backend
   - docker-compose.yml for local development
   - .env.example files
   - README with setup instructions
   - Postman collection with all API endpoints

**Output JSON Structure:**
{{
  "architecture": {{
    "frontend_pages": ["Home.tsx", "Dashboard.tsx", "Login.tsx", ...],
    "frontend_components": ["Button.tsx", "Modal.tsx", "Navbar.tsx", ...],
    "backend_routes": ["/api/auth/login", "/api/users", "/api/resources", ...],
    "database_tables": ["users", "resources", "permissions", ...]
  }},
  "database_schema": {{
    "tables": [
      {{
        "name": "users",
        "columns": [
          {{"name": "id", "type": "SERIAL PRIMARY KEY"}},
          {{"name": "email", "type": "VARCHAR(255) UNIQUE NOT NULL"}},
          {{"name": "password_hash", "type": "VARCHAR(255) NOT NULL"}},
          {{"name": "role", "type": "VARCHAR(50) DEFAULT 'user'"}},
          {{"name": "created_at", "type": "TIMESTAMP DEFAULT CURRENT_TIMESTAMP"}}
        ],
        "indexes": ["CREATE INDEX idx_users_email ON users(email);"]
      }},
      ...
    ]
  }},
  "api_endpoints": [
    {{"method": "POST", "path": "/api/auth/register", "description": "Create new user"}},
    {{"method": "POST", "path": "/api/auth/login", "description": "Login and get JWT"}},
    ...
  ],
  "file_list": ["frontend/src/pages/Home.tsx", "backend/src/routes/auth.ts", ...]
}}

Return ONLY valid JSON."""

        # Call AI for architecture (prefer Claude, fallback to GPT-4o)
        if has_anthropic:
            model = _default_anthropic_model()
            max_tokens = _get_model_max_tokens("anthropic", model)
            arch_response = await anthropic_client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=0.3,  # Low temp for structured thinking
                messages=[{
                    "role": "user",
                    "content": architecture_prompt
                }]
            )
            arch_text = arch_response.content[0].text.strip()
            arch_tokens = arch_response.usage.input_tokens + arch_response.usage.output_tokens
        else:
            # Use GPT-4o for architecture (reliable fallback)
            arch_response = await openai_client.chat.completions.create(
                model=_default_openai_model(),
                messages=[
                    {"role": "system", "content": "You are a senior software architect. Design complete production systems."},
                    {"role": "user", "content": architecture_prompt}
                ],
                temperature=0.3,
                max_tokens=16000,
                response_format={"type": "json_object"}
            )
            arch_text = arch_response.choices[0].message.content
            arch_tokens = arch_response.usage.total_tokens if arch_response.usage else 0
        
        architecture = sanitize_and_parse_json(arch_text, "Phase 1: Architecture")
        logger.info(f"‚úÖ Phase 1 complete: Architecture designed with {len(architecture.get('file_list', []))} files planned")
        
        # ==============================================
        # PHASE 2: FRONTEND CODE GENERATION (GPT-4o)
        # ==============================================
        logger.info(f"üé® Phase 2: GPT-4o generates frontend code...")
        
        frontend_files = architecture['architecture']['frontend_pages'] + architecture['architecture']['frontend_components']
        
        frontend_prompt = f"""Generate COMPLETE, production-ready {request.framework.value} frontend code.

**Architecture:** {json.dumps(architecture['architecture'], indent=2)}

**Generate ALL these files with full implementation:**
{json.dumps(frontend_files, indent=2)}

**Code Requirements:**
- TypeScript with strict typing
- {request.styling.value} for styling
- Axios for API calls with interceptors
- React Router for navigation
- JWT token management in localStorage
- Protected route components
- Form validation with error display
- Loading states and error boundaries
- Responsive design (mobile-first)
- Dark mode support
- NO PLACEHOLDERS - full working code

**Output JSON:**
{{
  "files": [
    {{"path": "frontend/src/pages/Home.tsx", "content": "Full TypeScript code here", "language": "typescript"}},
    {{"path": "frontend/src/components/Button.tsx", "content": "...", "language": "typescript"}},
    ...
  ]
}}

Return ONLY valid JSON."""

        frontend_response = await openai_client.chat.completions.create(
            model=_default_openai_model(),
            messages=[
                {"role": "system", "content": "You are an expert frontend developer. Generate COMPLETE production code with no placeholders."},
                {"role": "user", "content": frontend_prompt}
            ],
            temperature=0.7,
            max_tokens=16000,
            response_format={"type": "json_object"}
        )
        
        frontend_text = frontend_response.choices[0].message.content
        frontend_data = sanitize_and_parse_json(frontend_text, "Phase 2: Frontend")
        all_files = frontend_data.get('files', [])
        logger.info(f"‚úÖ Phase 2 complete: {len(all_files)} frontend files generated")
        
        # ==============================================
        # PHASE 3: BACKEND API GENERATION (GPT-4o)
        # ==============================================
        logger.info(f"‚öôÔ∏è Phase 3: GPT-4o generates backend API...")
        
        backend_prompt = f"""Generate COMPLETE Express.js + TypeScript backend API.

**Architecture:** {json.dumps(architecture['architecture'], indent=2)}
**Database Schema:** {json.dumps(architecture['database_schema'], indent=2)}
**API Endpoints:** {json.dumps(architecture['api_endpoints'], indent=2)}

**Generate ALL backend files:**
- src/index.ts (Express server setup)
- src/routes/*.ts (All API routes)
- src/middleware/auth.ts (JWT verification)
- src/middleware/validation.ts (Request validation)
- src/middleware/errorHandler.ts (Error handling)
- src/config/database.ts (PostgreSQL connection pool)
- src/controllers/*.ts (Business logic)
- src/models/*.ts (TypeScript interfaces)

**Code Requirements:**
- Full CRUD operations for all resources
- JWT authentication with refresh tokens
- Password hashing with bcrypt
- Input validation with express-validator
- Proper error handling and status codes
- PostgreSQL with pg library and connection pooling
- Environment variables
- CORS, helmet, rate-limiting
- API documentation comments
- NO PLACEHOLDERS - production-ready code

**Output JSON:**
{{
  "files": [
    {{"path": "backend/src/index.ts", "content": "Full TypeScript code", "language": "typescript"}},
    {{"path": "backend/src/routes/auth.ts", "content": "...", "language": "typescript"}},
    ...
  ]
}}

Return ONLY valid JSON."""

        backend_response = await openai_client.chat.completions.create(
            model=_default_openai_model(),
            messages=[
                {"role": "system", "content": "You are an expert backend developer. Generate COMPLETE production API code with no placeholders."},
                {"role": "user", "content": backend_prompt}
            ],
            temperature=0.7,
            max_tokens=16000,
            response_format={"type": "json_object"}
        )
        
        backend_text = backend_response.choices[0].message.content
        backend_data = sanitize_and_parse_json(backend_text, "Phase 3: Backend")
        all_files.extend(backend_data.get('files', []))
        logger.info(f"‚úÖ Phase 3 complete: {len(backend_data.get('files', []))} backend files generated")
        
        # ==============================================
        # PHASE 4: INTEGRATION & OPTIMIZATION
        # ==============================================
        logger.info(f"üîß Phase 4: {'Claude' if has_anthropic else 'GPT-4o'} integrates and adds deployment files...")
        
        integration_prompt = f"""Review and complete the full-stack application with deployment files.

**Generated Files So Far:** {len(all_files)} files

**Add these essential files:**
1. frontend/package.json (complete dependencies)
2. backend/package.json (complete dependencies)
3. database/schema.sql (full CREATE TABLE statements)
4. frontend/Dockerfile
5. backend/Dockerfile
6. docker-compose.yml (frontend + backend + postgres)
7. .env.example (frontend + backend)
8. README.md (complete setup guide)
9. postman_collection.json (all API endpoints)
10. frontend/tsconfig.json
11. backend/tsconfig.json
12. .gitignore
13. nginx.conf (optional reverse proxy)

**Database Schema SQL:**
{json.dumps(architecture['database_schema'], indent=2)}

**Generate ONLY these deployment/config files. Output JSON:**
{{
  "files": [
    {{"path": "frontend/package.json", "content": "...", "language": "json"}},
    {{"path": "database/schema.sql", "content": "CREATE TABLE statements", "language": "sql"}},
    ...
  ],
  "setup_instructions": "Detailed README content here",
  "dependencies": {{"frontend": {{}}, "backend": {{}}}}
}}

Return ONLY valid JSON."""

        if has_anthropic:
            model = _default_anthropic_model()
            max_tokens = _get_model_max_tokens("anthropic", model)
            integration_response = await anthropic_client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=0.5,
                messages=[{
                    "role": "user",
                    "content": integration_prompt
                }]
            )
            integration_text = integration_response.content[0].text.strip()
            integration_tokens = integration_response.usage.input_tokens + integration_response.usage.output_tokens
        else:
            integration_response = await openai_client.chat.completions.create(
                model=_default_openai_model(),
                messages=[
                    {"role": "system", "content": "You are an expert DevOps engineer. Generate complete deployment configurations."},
                    {"role": "user", "content": integration_prompt}
                ],
                temperature=0.5,
                max_tokens=16000,
                response_format={"type": "json_object"}
            )
            integration_text = integration_response.choices[0].message.content
            integration_tokens = integration_response.usage.total_tokens if integration_response.usage else 0
        
        integration_data = sanitize_and_parse_json(integration_text, "Phase 4: Integration")
        all_files.extend(integration_data.get('files', []))
        logger.info(f"‚úÖ Phase 4 complete: {len(integration_data.get('files', []))} deployment files added")
        
        # Calculate total tokens
        total_tokens = (
            arch_tokens +
            frontend_response.usage.total_tokens +
            backend_response.usage.total_tokens +
            integration_tokens
        )
        
        elapsed = int((time.time() - start_time) * 1000)
        logger.info(f"üéâ FULLSTACK GENERATION COMPLETE: {len(all_files)} files in {elapsed}ms")
        
        # Extract database schema SQL
        schema_file = next((f for f in all_files if 'schema.sql' in f['path']), None)
        database_schema = schema_file['content'] if schema_file else ""
        
        # ==============================================
        # PHASE 5: SAVE APP & AUTOMATIC DATABASE PROVISIONING
        # ==============================================
        database_info = None
        app_id = None
        
        if user_id != "anonymous" and http_client:
            try:
                logger.info(f"üíæ Phase 5: Saving app and provisioning database...")
                
                # Step 1: Save the generated app to Node API
                import uuid
                app_id = str(uuid.uuid4())
                app_name = request.description[:50].replace(" ", "-").lower()
                
                save_app_url = f"{API_BASE_URL}/v1/generated-apps"
                save_app_payload = {
                    "id": app_id,
                    "app_name": app_name,
                    "description": request.description,
                    "framework": request.framework.value,
                    "styling": request.styling.value,
                    "features": request.features or [],
                    "files": [
                        {
                            "file_path": f["path"],
                            "content": f["content"],
                            "language": f.get("language", "typescript")
                        }
                        for f in all_files
                    ],
                    "dependencies": integration_data.get('dependencies', {}),
                    "requires_database": True,
                    "status": "complete"
                }
                
                try:
                    logger.info(f"   ‚Üí Saving app with {len(all_files)} files...")
                    save_response = await http_client.post(
                        save_app_url,
                        json=save_app_payload,
                        timeout=30.0
                    )
                    
                    if save_response.status_code in [200, 201]:
                        logger.info(f"   ‚úÖ App saved successfully: {app_id}")
                        
                        # Step 2: Provision database for the app
                        if database_schema:
                            logger.info(f"   ‚Üí Provisioning database with automatic schema execution...")
                            provision_url = f"{API_BASE_URL}/v1/generated-apps/{app_id}/database/provision"
                            
                            provision_response = await http_client.post(
                                provision_url,
                                json={"initialize_schema": True},
                                timeout=20.0
                            )
                            
                            if provision_response.status_code == 200:
                                provision_data = provision_response.json()
                                database_info = provision_data.get('database', {})
                                tables_created = provision_data.get('tables_created', 0)
                                
                                logger.info(f"   ‚úÖ Database provisioned: {database_info.get('database', 'unknown')} with {tables_created} tables created!")
                                
                                # Add database connection info to response
                                database_info['tables_created'] = tables_created
                                database_info['connection_string'] = provision_data.get('connection_string', '')
                            else:
                                logger.warning(f"   ‚ö†Ô∏è  Database provisioning returned {provision_response.status_code}: {provision_response.text[:200]}")
                    else:
                        logger.warning(f"   ‚ö†Ô∏è  App save returned {save_response.status_code}: {save_response.text[:200]}")
                
                except Exception as api_error:
                    logger.error(f"   ‚ùå API call failed: {str(api_error)}")
                    # Continue without saving - user gets files anyway
                
            except Exception as provision_error:
                logger.error(f"Phase 5 error (non-fatal): {str(provision_error)}")
                # Continue - database provisioning is optional enhancement
        
        # Build response
        response_data = {
            "files": all_files,
            "instructions": integration_data.get('setup_instructions', 'See README.md for setup instructions'),
            "dependencies": integration_data.get('dependencies', {}),
            "requires_database": True,
            "database_schema": database_schema,
            "database_info": database_info,  # Include provisioned database credentials
            "app_id": app_id,  # Include app ID if saved
            "deployment_config": {
                "framework": request.framework.value,
                "port": 3000,
                "build_command": "npm run build",
                "start_command": "npm start"
            },
            "provider_used": "dual-ai/claude-3.5-sonnet+gpt-4o" if has_anthropic else "openai/gpt-4o",
            "generation_time_ms": elapsed,
            "tokens_used": total_tokens
        }
        
        # Send N8N webhook (async, non-blocking)
        asyncio.create_task(send_n8n_webhook(N8N_APP_GENERATED, {
            "event": "fullstack_app_generated",
            "user_id": user_id,
            "app_id": app_id,
            "framework": request.framework.value,
            "provider": "dual-ai" if has_anthropic else "openai",
            "files_count": len(response_data["files"]),
            "requires_database": response_data["requires_database"],
            "database_provisioned": database_info is not None,
            "tables_created": database_info.get('tables_created', 0) if database_info else 0,
            "generated_at": datetime.utcnow().isoformat()
        }))
        
        logger.info(f"üéâ FULLSTACK APP COMPLETE: {len(response_data['files'])} files | Database: {'‚úÖ PROVISIONED with ' + str(database_info.get('tables_created', 0)) + ' tables' if database_info else 'üìÑ Schema only'}")
        
        return MultiFileAppResponse(**response_data)
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON parsing error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AI returned invalid JSON: {str(e)}")
    except Exception as e:
        logger.error(f"Fullstack generation error: {str(e)}", exc_info=True)
        
        # Send error webhook
        if N8N_APP_ERROR:
            asyncio.create_task(send_n8n_webhook(N8N_APP_ERROR, {
                "error": str(e),
                "description": request.description,
                "timestamp": datetime.utcnow().isoformat()
            }))
        
        raise HTTPException(status_code=500, detail=str(e))

# ============================================
# ERROR HANDLERS
# ============================================

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handle HTTP exceptions"""
    logger.error(f"HTTP {exc.status_code}: {exc.detail}")
    return JSONResponse(
        status_code=exc.status_code,
        content={"error": exc.detail}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """Handle general exceptions"""
    logger.error(f"Unhandled exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"error": "Internal server error"}
    )

# ============================================
# RUN SERVER
# ============================================

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "5001"))
    uvicorn.run(
        "app_nexusai_production:app",
        host="0.0.0.0",
        port=port,
        reload=True,
        log_level="info"
    )
