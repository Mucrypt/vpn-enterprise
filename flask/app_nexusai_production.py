"""
VPN Enterprise - Production AI API (Enterprise Grade)
=========================================================
World-class AI code generation powered by OpenAI GPT-4o & Anthropic Claude 3.5 Sonnet
Integrated with N8N for automated deployment, testing, and monitoring

Features:
- Dual AI Provider Support (OpenAI + Anthropic) with intelligent routing
- N8N Webhook Integration for CI/CD automation
- Enterprise-grade rate limiting, caching, and monitoring
- Automated database provisioning for generated apps
- Security scanning and code quality checks
- Real-time deployment status tracking

Author: VPN Enterprise Team
Version: 2.0.0 (Production)
"""

from fastapi import FastAPI, HTTPException, status, Request, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, ValidationError
from datetime import datetime, timedelta
from contextlib import asynccontextmanager
from typing import Optional, Dict, Any, List, Literal
import httpx
import os
import logging
import hashlib
import json
import time
import asyncio
import re
from enum import Enum

# AI Provider imports
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from json_repair import repair_json

# Redis for async job queue
import redis.asyncio as redis

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============================================
# HELPER FUNCTIONS
# ============================================

def read_secret(env_var_name: str, file_env_var_name: str) -> str:
    """
    Read secret from file if *_FILE env var is set, otherwise from direct env var.
    This follows Docker secrets pattern used throughout the application.
    """
    # First check if there's a *_FILE env var pointing to a secret file
    secret_file_path = os.getenv(file_env_var_name, "")
    if secret_file_path and os.path.exists(secret_file_path):
        try:
            with open(secret_file_path, 'r') as f:
                secret = f.read().strip()
                if secret:
                    logger.info(f"‚úÖ Loaded {env_var_name} from secret file: {secret_file_path}")
                    return secret
        except Exception as e:
            logger.error(f"‚ùå Failed to read {secret_file_path}: {e}")
    
    # Fallback to direct env var
    direct_value = os.getenv(env_var_name, "")
    if direct_value:
        logger.info(f"‚úÖ Loaded {env_var_name} from environment variable")
    return direct_value

def sanitize_and_parse_json(text: str, context: str = "AI response") -> dict:
    """
    Enterprise-grade JSON sanitization and parsing for AI-generated content.
    Multi-layer approach: clean ‚Üí parse ‚Üí repair ‚Üí parse ‚Üí fail with details.
    
    Args:
        text: Raw text response from AI
        context: Description for error messages
    
    Returns:
        Parsed JSON dict
    
    Raises:
        ValueError: If JSON cannot be parsed even after all repair attempts
    """
    try:
        # Layer 1: Remove markdown code fences
        cleaned = text.strip()
        if cleaned.startswith("```"):
            cleaned = re.sub(r'```json?\n?', '', cleaned)
            cleaned = re.sub(r'\n?```$', '', cleaned)
        
        # Layer 2: Try fast-path parsing (works for 90% of cases)
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            pass
        
        # Layer 3: Aggressive control character sanitization
        # Remove or escape control characters (0x00-0x1F except whitespace)
        def clean_control_chars(match):
            char = match.group(0)
            code = ord(char)
            if code == 0x09:  # tab
                return '\\t'
            elif code == 0x0A:  # newline
                return '\\n'
            elif code == 0x0D:  # carriage return
                return '\\r'
            else:  # other control chars - remove completely
                return ''
        
        cleaned = re.sub(r'[\x00-\x1F]', clean_control_chars, cleaned)
        
        # Layer 4: Try parsing with cleaned control characters
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            pass
        
        # Layer 5: Use json-repair as fallback (enterprise-grade recovery)
        try:
            logger.info(f"Attempting JSON repair for {context}...")
            repaired = repair_json(cleaned, return_objects=True)
            if isinstance(repaired, dict):
                logger.info(f"‚úÖ JSON repair successful for {context}")
                return repaired
            else:
                # repair_json returned string, parse it
                return json.loads(repaired)
        except Exception as repair_error:
            logger.warning(f"JSON repair failed for {context}: {str(repair_error)}")
        
        # Layer 6: Final attempt - detailed error reporting
        try:
            json.loads(cleaned)
        except json.JSONDecodeError as e:
            error_pos = e.pos if hasattr(e, 'pos') else 0
            snippet_start = max(0, error_pos - 150)
            snippet_end = min(len(cleaned), error_pos + 150)
            problematic = cleaned[snippet_start:snippet_end]
            
            logger.error(f"‚ùå JSON parse failed in {context} at position {error_pos}")
            logger.error(f"Problematic snippet: ...{problematic}...")
            logger.error(f"Error: {str(e)}")
            
            raise ValueError(
                f"AI returned invalid JSON in {context}: {str(e)}. "
                f"Position: {error_pos}. Check logs for details."
            )
            
    except ValueError:
        raise  # Re-raise ValueError with our custom message
    except Exception as e:
        logger.error(f"Unexpected error in JSON sanitization for {context}: {str(e)}")
        raise ValueError(f"Failed to process JSON from {context}: {str(e)}")

# ============================================
# CONFIGURATION
# ============================================

# AI Provider Keys (support both direct env vars and Docker secrets)
OPENAI_API_KEY = read_secret("OPENAI_API_KEY", "OPENAI_API_KEY_FILE")
ANTHROPIC_API_KEY = read_secret("ANTHROPIC_API_KEY", "ANTHROPIC_API_KEY_FILE")

# N8N Webhook Configuration
N8N_WEBHOOK_BASE = os.getenv("N8N_WEBHOOK_URL", "https://chatbuilds.com/webhook")
N8N_APP_GENERATED = f"{N8N_WEBHOOK_BASE}/nexusai-app-generated"
N8N_APP_DEPLOY = f"{N8N_WEBHOOK_BASE}/nexusai-deploy"
N8N_APP_ERROR = f"{N8N_WEBHOOK_BASE}/nexusai-error"

# Service URLs
API_BASE_URL = os.getenv("API_URL", "https://chatbuilds.com/api")
DATABASE_PROVISIONER_URL = os.getenv("DATABASE_PROVISIONER_URL", "http://localhost:3003")

# Redis Configuration (for async job queue)
REDIS_URL = os.getenv("REDIS_URL", "redis://vpn-redis:6379")
redis_client: Optional[redis.Redis] = None

# Cache & Rate Limiting
CACHE_TTL = 3600  # 1 hour
memory_cache: Dict[str, Any] = {}
rate_limit_store: Dict[str, List[float]] = {}

# Rate limits by subscription tier
RATE_LIMITS = {
    "free": {"ai": 10, "deploy": 5, "window": 3600},      # 10 AI calls/hour, 5 deploys/day
    "pro": {"ai": 100, "deploy": 50, "window": 3600},     # 100 AI calls/hour, 50 deploys/day
    "enterprise": {"ai": 1000, "deploy": 500, "window": 3600}  # Unlimited
}

# AI Model Configuration
OPENAI_MODELS = {
    "gpt-4o": {"cost": 10, "max_tokens": 16384, "best_for": "complex apps"},
    "gpt-4o-mini": {"cost": 5, "max_tokens": 16384, "best_for": "simple apps"},
    "gpt-3.5-turbo": {"cost": 3, "max_tokens": 4096, "best_for": "quick prototypes"}
}

ANTHROPIC_MODELS = {
    "claude-3-5-sonnet-20241022": {"cost": 10, "max_tokens": 8192, "best_for": "balanced"},
    "claude-3-5-sonnet-20240620": {"cost": 10, "max_tokens": 8192, "best_for": "legacy"},
    "claude-3-opus-20240229": {"cost": 12, "max_tokens": 8192, "best_for": "production code"},
    "claude-3-haiku-20240307": {"cost": 5, "max_tokens": 4096, "best_for": "fast generation"}
}

# Initialize clients
openai_client = None
anthropic_client = None
http_client: Optional[httpx.AsyncClient] = None

if OPENAI_API_KEY:
    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    logger.info("‚úÖ OpenAI client initialized with GPT-4o access")
    
if ANTHROPIC_API_KEY:
    anthropic_client = AsyncAnthropic(api_key=ANTHROPIC_API_KEY)
    logger.info("‚úÖ Anthropic client initialized with Claude 3.5 Sonnet access")

# ============================================
# LIFESPAN & APP INITIALIZATION
# ============================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan - startup and shutdown"""
    global http_client, redis_client
    
    logger.info("=" * 60)
    logger.info("üöÄ VPN ENTERPRISE AI API - PRODUCTION MODE")
    logger.info("=" * 60)
    logger.info(f"üìÖ Started: {datetime.utcnow().isoformat()}")
    logger.info(f"ü§ñ AI Providers Active:")
    if openai_client:
        logger.info("   ‚úÖ OpenAI GPT-4o (Industry Leading)")
    if anthropic_client:
        logger.info("   ‚úÖ Anthropic Claude 3.5 Sonnet (Code Specialist)")
    
    logger.info(f"üîó N8N Webhooks:")
    logger.info(f"   ‚Ä¢ App Generated: {N8N_APP_GENERATED}")
    logger.info(f"   ‚Ä¢ App Deploy: {N8N_APP_DEPLOY}")
    logger.info(f"   ‚Ä¢ App Error: {N8N_APP_ERROR}")
    
    if not openai_client and not anthropic_client:
        # Don't crash the container in production. Keep the service up so
        # Nginx health checks stay green and callers get a clear 503.
        logger.error("‚ùå NO AI PROVIDERS! Set OPENAI_API_KEY or ANTHROPIC_API_KEY")
    
    # Initialize HTTP client for webhooks
    http_client = httpx.AsyncClient(timeout=10.0)
    
    # Initialize Redis for async job queue
    try:
        redis_client = await redis.from_url(
            REDIS_URL,
            encoding="utf-8",
            decode_responses=True,
            socket_connect_timeout=5
        )
        await redis_client.ping()
        logger.info(f"‚úÖ Redis connected: {REDIS_URL.split('@')[-1]} (Async Job Queue)")
    except Exception as e:
        logger.error(f"‚ùå Redis connection failed: {e}")
        logger.warning("‚ö†Ô∏è  Job queue will operate in memory-only mode (not recommended for production)")
    logger.info("‚úÖ HTTP client initialized for N8N webhooks")
    logger.info("=" * 60)
    
    yield
    
    # Shutdown
    if http_client:
        await http_client.aclose()
    if redis_client:
        await redis_client.aclose()
        logger.info("‚úÖ Redis connection closed")
    logger.info("üõë API shutting down gracefully")

app = FastAPI(
    title="VPN Enterprise AI API",
    description="Enterprise-grade AI code generation with OpenAI & Anthropic",
    version="2.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# MODELS & SCHEMAS
# ============================================

class AIProvider(str, Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    AUTO = "auto"  # Intelligent routing

class AppFramework(str, Enum):
    REACT = "react"
    VUE = "vue"
    NEXTJS = "nextjs"
    EXPRESS = "express"
    FASTAPI = "fastapi"
    FLASK = "flask"

class StylingFramework(str, Enum):
    TAILWIND = "tailwind"
    CSS = "css"
    STYLED_COMPONENTS = "styled-components"
    MUI = "mui"

class AIGenerateRequest(BaseModel):
    prompt: str = Field(..., min_length=10, max_length=100000)
    model: Optional[str] = Field(default=None)
    provider: AIProvider = Field(default=AIProvider.AUTO)
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    max_tokens: int = Field(default=8192, le=16384)

class MultiFileAppRequest(BaseModel):
    description: str = Field(..., min_length=20, max_length=10000)
    framework: AppFramework = Field(default=AppFramework.REACT)
    styling: StylingFramework = Field(default=StylingFramework.TAILWIND)
    features: List[str] = Field(default_factory=list)
    provider: AIProvider = Field(default=AIProvider.AUTO)
    include_database: bool = Field(default=True)
    include_auth: bool = Field(default=False)
    include_api: bool = Field(default=True)

class FileOutput(BaseModel):
    path: str
    content: str
    language: str

class JobStatus(str, Enum):
    """Job states for async generation"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class JobPhase(str, Enum):
    """5-phase fullstack generation tracking"""
    ARCHITECTURE = "architecture"      # Phase 1: Planning
    FRONTEND = "frontend"              # Phase 2: UI
    BACKEND = "backend"                # Phase 3: API
    INTEGRATION = "integration"        # Phase 4: Config
    DATABASE = "database"              # Phase 5: Schema + save

class JobInfo(BaseModel):
    """Job metadata for async fullstack generation"""
    job_id: str
    status: JobStatus
    phase: Optional[JobPhase] = None
    progress_percent: int = Field(ge=0, le=100)
    message: str = ""
    created_at: datetime
    updated_at: datetime
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class MultiFileAppResponse(BaseModel):
    files: List[FileOutput]
    instructions: str
    dependencies: Dict[str, str]
    requires_database: bool
    database_schema: Optional[str] = None
    database_info: Optional[Dict[str, Any]] = None  # Database connection credentials and info
    app_id: Optional[str] = None  # Saved app ID if persisted
    deployment_config: Optional[Dict[str, Any]] = None
    provider_used: str
    generation_time_ms: int
    tokens_used: int

class DeploymentRequest(BaseModel):
    model_config = {"extra": "allow"}

    app_name: str
    files: List[FileOutput]
    dependencies: Dict[str, str] = Field(default_factory=dict)
    framework: str = Field(default="react")
    requires_database: bool
    database_schema: Optional[str] = None
    user_id: str = Field(default="anonymous")
    user_email: Optional[str] = None
    app_id: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    timestamp: datetime
    version: str
    ai_providers: Dict[str, bool]
    n8n_enabled: bool

class ModelsResponse(BaseModel):
    models: List[str]

class SQLAssistRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=100000)
    action: str = Field(..., pattern="^(generate|explain|optimize|fix)$")
    schema: Optional[str] = None
    sql: Optional[str] = None
    provider: AIProvider = Field(default=AIProvider.AUTO)
    model: Optional[str] = None

class SQLAssistResponse(BaseModel):
    sql: Optional[str] = None
    explanation: Optional[str] = None
    optimized: Optional[str] = None
    fixed: Optional[str] = None
    suggestions: Optional[List[str]] = None
    provider_used: Optional[str] = None
    model_used: Optional[str] = None

class AIProvidersResponse(BaseModel):
    providers: List[Dict[str, str]]
    routing_strategy: str

class DeployAppRequest(BaseModel):
    app_name: str = Field(..., min_length=1, max_length=80)
    files: List[FileOutput]
    dependencies: Dict[str, str] = Field(default_factory=dict)
    framework: str = Field(default="react")
    requires_database: bool = Field(default=True)
    user_id: str = Field(default="anonymous")
    # Optional fields used by other flows
    app_id: Optional[str] = None
    database_schema: Optional[str] = None
    user_email: Optional[str] = None

class DeploymentStatusResponse(BaseModel):
    deployment_id: str
    status: str
    progress: int
    current_step: str
    logs: List[str]
    error: Optional[str] = None

class DeploymentResponse(BaseModel):
    deployment_id: str
    app_name: str
    status: str
    database: Optional[Dict[str, Any]] = None
    hosting: Optional[Dict[str, Any]] = None
    app_url: Optional[str] = None
    environment: Optional[Dict[str, str]] = None
    steps: List[Dict[str, str]]

# ============================================
# UTILITY FUNCTIONS
# ============================================

def cache_key(prefix: str, *args: Any) -> str:
    """Generate cache key"""
    data = json.dumps(args, sort_keys=True)
    hash_val = hashlib.md5(data.encode()).hexdigest()
    return f"{prefix}:{hash_val}"

def get_cache(key: str) -> Optional[Any]:
    """Get from cache"""
    if key in memory_cache:
        entry = memory_cache[key]
        if entry["expires"] > time.time():
            logger.info(f"üíæ Cache HIT: {key}")
            return entry["data"]
        else:
            del memory_cache[key]
    return None

def set_cache(key: str, data: Any, ttl: int = CACHE_TTL):
    """Set cache"""
    memory_cache[key] = {
        "data": data,
        "expires": time.time() + ttl
    }
    logger.info(f"üíæ Cache SET: {key} (TTL: {ttl}s)")

def check_rate_limit(user_id: str, tier: str, limit_type: str = "ai") -> bool:
    """Check if user is within rate limits"""
    if tier not in RATE_LIMITS:
        tier = "free"
    
    limits = RATE_LIMITS[tier]
    limit = limits.get(limit_type, limits["ai"])
    window = limits["window"]
    
    key = f"{user_id}:{limit_type}"
    now = time.time()
    
    if key not in rate_limit_store:
        rate_limit_store[key] = []
    
    # Remove old entries
    rate_limit_store[key] = [t for t in rate_limit_store[key] if now - t < window]
    
    if len(rate_limit_store[key]) >= limit:
        logger.warning(f"‚ö†Ô∏è  Rate limit exceeded for {user_id} ({tier}): {len(rate_limit_store[key])}/{limit}")
        return False
    
    rate_limit_store[key].append(now)
    return True

async def send_n8n_webhook(webhook_url: str, payload: Dict[str, Any]):
    """Send webhook to N8N (fire and forget)"""
    try:
        if http_client:
            await http_client.post(webhook_url, json=payload, timeout=5.0)
            logger.info(f"üîî N8N webhook sent: {webhook_url}")
    except Exception as e:
        logger.error(f"‚ùå N8N webhook failed: {str(e)}")

def choose_ai_provider(description: str, provider: AIProvider) -> tuple[str, Any]:
    """Intelligently choose AI provider"""
    if provider == AIProvider.OPENAI and openai_client:
        return ("openai", openai_client)
    
    if provider == AIProvider.ANTHROPIC and anthropic_client:
        return ("anthropic", anthropic_client)
    
    # AUTO mode - intelligent routing
    if provider == AIProvider.AUTO:
        # Prefer OpenAI GPT-4o as primary (most reliable and widely available)
        if openai_client:
            logger.info("üß† AUTO: Chose GPT-4o (primary provider)")
            return ("openai", openai_client)
        
        # Fallback to Claude if OpenAI unavailable
        if anthropic_client:
            logger.info("üß† AUTO: Chose Claude 3.5 Sonnet (fallback)")
            return ("anthropic", anthropic_client)
    
    raise HTTPException(
        status_code=503,
        detail="No AI provider available for the requested provider type"
    )

# ============================================
# ASYNC JOB QUEUE MANAGEMENT (Redis-backed)
# ============================================

async def create_job(user_id: str, request_data: Dict[str, Any]) -> str:
    """Create a new async job and return job_id immediately"""
    import uuid
    job_id = str(uuid.uuid4())
    
    now = datetime.utcnow()
    job_data = {
        "job_id": job_id,
        "user_id": user_id,
        "status": JobStatus.PENDING.value,
        "phase": None,
        "progress_percent": 0,
        "message": "Job queued, waiting to start...",
        "created_at": now.isoformat(),
        "updated_at": now.isoformat(),
        "request_data": json.dumps(request_data),
        "result": None,
        "error": None
    }
    
    if redis_client:
        try:
            # Store job in Redis with 24-hour TTL
            await redis_client.hset(f"job:{job_id}", mapping=job_data)
            await redis_client.expire(f"job:{job_id}", 86400)  # 24 hours
            logger.info(f"‚úÖ Job created in Redis: {job_id}")
        except Exception as e:
            logger.error(f"‚ùå Redis job creation failed: {e}, using memory fallback")
            memory_cache[f"job:{job_id}"] = {"data": job_data, "expires": time.time() + 86400}
    else:
        # Fallback to memory
        memory_cache[f"job:{job_id}"] = {"data": job_data, "expires": time.time() + 86400}
        logger.warning(f"‚ö†Ô∏è  Job {job_id} created in memory (Redis unavailable)")
    
    return job_id

async def update_job_progress(job_id: str, phase: JobPhase, progress: int, message: str):
    """Update job progress during generation"""
    update_data = {
        "status": JobStatus.RUNNING.value,
        "phase": phase.value,
        "progress_percent": progress,
        "message": message,
        "updated_at": datetime.utcnow().isoformat()
    }
    
    if redis_client:
        try:
            await redis_client.hset(f"job:{job_id}", mapping=update_data)
            logger.info(f"üìä Job {job_id}: {phase.value} ({progress}%) - {message}")
        except Exception as e:
            logger.error(f"‚ùå Redis update failed for {job_id}: {e}")
            # Update memory fallback
            if f"job:{job_id}" in memory_cache:
                memory_cache[f"job:{job_id}"]["data"].update(update_data)
    else:
        # Memory fallback
        if f"job:{job_id}" in memory_cache:
            memory_cache[f"job:{job_id}"]["data"].update(update_data)

async def complete_job(job_id: str, result: Dict[str, Any]):
    """Mark job as completed with final result"""
    completion_data = {
        "status": JobStatus.COMPLETED.value,
        "progress_percent": 100,
        "message": "Generation completed successfully!",
        "updated_at": datetime.utcnow().isoformat(),
        "result": json.dumps(result)
    }
    
    if redis_client:
        try:
            await redis_client.hset(f"job:{job_id}", mapping=completion_data)
            logger.info(f"‚úÖ Job {job_id} completed successfully")
        except Exception as e:
            logger.error(f"‚ùå Redis completion failed for {job_id}: {e}")
            if f"job:{job_id}" in memory_cache:
                memory_cache[f"job:{job_id}"]["data"].update(completion_data)
    else:
        if f"job:{job_id}" in memory_cache:
            memory_cache[f"job:{job_id}"]["data"].update(completion_data)

async def fail_job(job_id: str, error: str):
    """Mark job as failed with error message"""
    failure_data = {
        "status": JobStatus.FAILED.value,
        "message": "Generation failed",
        "updated_at": datetime.utcnow().isoformat(),
        "error": error
    }
    
    if redis_client:
        try:
            await redis_client.hset(f"job:{job_id}", mapping=failure_data)
            logger.error(f"‚ùå Job {job_id} failed: {error}")
        except Exception as e:
            logger.error(f"‚ùå Redis failure update failed for {job_id}: {e}")
            if f"job:{job_id}" in memory_cache:
                memory_cache[f"job:{job_id}"]["data"].update(failure_data)
    else:
        if f"job:{job_id}" in memory_cache:
            memory_cache[f"job:{job_id}"]["data"].update(failure_data)

async def get_job_status(job_id: str) -> Optional[JobInfo]:
    """Get current job status for polling"""
    job_data = None
    
    if redis_client:
        try:
            data = await redis_client.hgetall(f"job:{job_id}")
            if data:
                job_data = data
        except Exception as e:
            logger.error(f"‚ùå Redis get failed for {job_id}: {e}")
    
    # Fallback to memory
    if not job_data and f"job:{job_id}" in memory_cache:
        entry = memory_cache[f"job:{job_id}"]
        if entry["expires"] > time.time():
            job_data = entry["data"]
    
    if not job_data:
        return None
    
    # Parse result and error from JSON if present
    result = None
    if job_data.get("result"):
        try:
            result = json.loads(job_data["result"])
        except:
            result = job_data["result"]
    
    return JobInfo(
        job_id=job_data["job_id"],
        status=JobStatus(job_data["status"]),
        phase=JobPhase(job_data["phase"]) if job_data.get("phase") else None,
        progress_percent=int(job_data.get("progress_percent", 0)),
        message=job_data.get("message", ""),
        created_at=datetime.fromisoformat(job_data["created_at"]),
        updated_at=datetime.fromisoformat(job_data["updated_at"]),
        result=result,
        error=job_data.get("error")
    )

def _default_openai_model() -> str:
    return "gpt-4o"

def _default_anthropic_model() -> str:
    # Use Claude 3 Haiku - fast, reliable, widely available
    # Keep this aligned with ANTHROPIC_MODELS.
    return "claude-3-haiku-20240307"

def _get_model_max_tokens(provider_name: str, model: str) -> int:
    """Get the appropriate max_tokens for a model."""
    if provider_name == "anthropic":
        model_info = ANTHROPIC_MODELS.get(model, {})
        return model_info.get("max_tokens", 4096)  # Default to 4096 for safety
    else:  # openai
        model_info = OPENAI_MODELS.get(model, {})
        return model_info.get("max_tokens", 16000)

def _normalize_requested_model(provider_name: str, requested_model: Optional[str]) -> str:
    """Avoid passing invalid/placeholder model names upstream.

    The NexusAI frontend historically used non-OpenAI/Anthropic defaults (e.g. Ollama
    names like deepseek-coder-v2). Those should safely fall back to a valid model.
    """
    if not requested_model:
        return _default_openai_model() if provider_name == "openai" else _default_anthropic_model()

    requested_model = requested_model.strip()
    if requested_model in {"auto", "default"}:
        return _default_openai_model() if provider_name == "openai" else _default_anthropic_model()

    if provider_name == "openai":
        if requested_model in OPENAI_MODELS:
            return requested_model
        # Allow direct OpenAI model names even if not listed.
        if requested_model.startswith("gpt-"):
            return requested_model
        return _default_openai_model()

    # anthropic
    if requested_model in ANTHROPIC_MODELS:
        return requested_model
    if requested_model.startswith("claude-"):
        return requested_model
    return _default_anthropic_model()

_deployment_status_store: Dict[str, DeploymentStatusResponse] = {}

# ============================================
# PROMPT TEMPLATES
# ============================================

def get_app_generation_prompt(request: MultiFileAppRequest) -> str:
    """Generate comprehensive prompt for app generation"""
    
    features_text = ", ".join(request.features) if request.features else "basic CRUD operations"
    
    prompt = f"""Generate a complete, production-ready {request.framework.value} application with the following requirements:

**Project Description:**
{request.description}

**Technical Stack:**
- Framework: {request.framework.value}
- Styling: {request.styling.value}
- Features: {features_text}
- Database: {"PostgreSQL (with schema)" if request.include_database else "None"}
- Authentication: {"Yes (JWT-based)" if request.include_auth else "No"}
- API Layer: {"Yes (REST API)" if request.include_api else "No"}

**Requirements:**
1. Generate COMPLETE source files (no placeholders or TODO comments)
2. Include proper error handling and validation
3. Use modern best practices and patterns
4. Add TypeScript types where applicable
5. Include package.json with all dependencies
6. Add README with setup instructions
7. Include environment variable template (.env.example)
8. Add Dockerfile for containerization
9. {"Generate PostgreSQL database schema with tables, indexes, and constraints" if request.include_database else ""}

**Output Format:**
Return a JSON object with this EXACT structure:
{{
    "files": [
        {{"path": "src/App.tsx", "content": "...", "language": "typescript"}},
        {{"path": "package.json", "content": "...", "language": "json"}},
        ...
    ],
    "instructions": "Step-by-step setup instructions",
    "dependencies": {{"react": "^18.0.0", ...}},
    "requires_database": true/false,
    "database_schema": "CREATE TABLE statements..." (if database is needed)
}}

**CRITICAL:** Return ONLY the JSON object, no markdown, no explanations, just pure JSON.
"""
    
    return prompt

# ============================================
# API ENDPOINTS
# ============================================

@app.get("/", response_model=Dict[str, Any])
async def root():
    """Root endpoint"""
    return {
        "service": "VPN Enterprise AI API",
        "version": "2.0.0",
        "status": "operational",
        "ai_providers": {
            "openai": openai_client is not None,
            "anthropic": anthropic_client is not None
        },
        "endpoints": {
            "health": "/health",
            "docs": "/docs",
            "generate": "/ai/generate",
            "generate_app": "/ai/generate/app",
            "deploy": "/ai/deploy/app"
        }
    }

@app.get("/health", response_model=HealthResponse)
@app.get("/ai/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    any_provider = (openai_client is not None) or (anthropic_client is not None)
    return HealthResponse(
        status="healthy" if any_provider else "degraded",
        timestamp=datetime.utcnow(),
        version="2.0.0",
        ai_providers={
            "openai": openai_client is not None,
            "anthropic": anthropic_client is not None
        },
        n8n_enabled=bool(N8N_WEBHOOK_BASE)
    )

@app.get("/models", response_model=ModelsResponse)
@app.get("/ai/models", response_model=ModelsResponse)
async def list_models():
    """List models supported by configured providers."""
    models: List[str] = []
    if openai_client is not None:
        models.extend(list(OPENAI_MODELS.keys()))
    if anthropic_client is not None:
        models.extend(list(ANTHROPIC_MODELS.keys()))

    # If no providers configured, still return a stable response.
    if not models:
        models = list(OPENAI_MODELS.keys()) + list(ANTHROPIC_MODELS.keys())

    # Deduplicate while keeping order.
    seen = set()
    unique_models: List[str] = []
    for m in models:
        if m in seen:
            continue
        unique_models.append(m)
        seen.add(m)

    return ModelsResponse(models=unique_models)

@app.get("/ai/providers", response_model=AIProvidersResponse)
@app.get("/ai/ai/providers", response_model=AIProvidersResponse)
async def ai_providers():
    """Expose provider availability for the frontend UI."""
    providers: List[Dict[str, str]] = []
    providers.append({
        "name": "OpenAI",
        "model": _default_openai_model(),
        "status": "available" if openai_client is not None else "unconfigured",
    })
    providers.append({
        "name": "Anthropic",
        "model": _default_anthropic_model(),
        "status": "available" if anthropic_client is not None else "unconfigured",
    })
    return AIProvidersResponse(providers=providers, routing_strategy="auto")

@app.post("/sql/assist", response_model=SQLAssistResponse)
@app.post("/ai/sql/assist", response_model=SQLAssistResponse)
async def sql_assist(request: SQLAssistRequest):
    """SQL assistance endpoint used by the NexusAI frontend."""
    provider_name, client = choose_ai_provider(request.query, request.provider)
    model = _normalize_requested_model(provider_name, request.model)

    schema_text = f"\n\nSchema:\n{request.schema}" if request.schema else ""
    sql_text = f"\n\nSQL:\n{request.sql}" if request.sql else ""

    if request.action == "generate":
        prompt = f"Generate PostgreSQL SQL for this request:\n{request.query}{schema_text}\nReturn ONLY SQL."
    elif request.action == "explain":
        prompt = f"Explain this SQL in plain English, including performance considerations.\n{request.query}{schema_text}{sql_text}"
    elif request.action == "optimize":
        prompt = f"Optimize this SQL for PostgreSQL. Provide optimized SQL only.\n{request.query}{schema_text}{sql_text}"
    else:  # fix
        prompt = f"Fix this SQL for PostgreSQL. Provide fixed SQL only.\n{request.query}{schema_text}{sql_text}"

    try:
        if provider_name == "openai":
            response = await client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=2048,
            )
            text = response.choices[0].message.content
        else:
            response = await client.messages.create(
                model=model,
                max_tokens=2048,
                temperature=0.2,
                messages=[{"role": "user", "content": prompt}],
            )
            text = response.content[0].text

        text = (text or "").strip()
        if request.action == "explain":
            return SQLAssistResponse(
                explanation=text,
                provider_used=provider_name,
                model_used=model,
            )
        if request.action == "optimize":
            return SQLAssistResponse(
                optimized=text,
                provider_used=provider_name,
                model_used=model,
            )
        if request.action == "fix":
            return SQLAssistResponse(
                fixed=text,
                provider_used=provider_name,
                model_used=model,
            )
        return SQLAssistResponse(sql=text, provider_used=provider_name, model_used=model)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå SQL assist failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"SQL assist failed: {str(e)}")

@app.post("/ai/generate", response_model=Dict[str, Any])
async def generate_ai_text(
    request: AIGenerateRequest,
    x_api_key: Optional[str] = Header(None)
):
    """Generate AI text/code"""
    start_time = time.time()
    
    # Choose provider
    provider_name, client = choose_ai_provider(request.prompt, request.provider)
    
    try:
        if provider_name == "openai":
            model = _normalize_requested_model(provider_name, request.model)
            response = await client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": request.prompt}],
                temperature=request.temperature,
                max_tokens=request.max_tokens
            )
            result = response.choices[0].message.content
            tokens = response.usage.total_tokens if response.usage else 0
            
        else:  # anthropic
            model = _normalize_requested_model(provider_name, request.model)
            response = await client.messages.create(
                model=model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                messages=[{"role": "user", "content": request.prompt}]
            )
            result = response.content[0].text
            tokens = response.usage.input_tokens + response.usage.output_tokens if hasattr(response, 'usage') else 0
        
        elapsed = int((time.time() - start_time) * 1000)
        
        return {
            "response": result,
            "provider": provider_name,
            "model": model,
            "tokens_used": tokens,
            "generation_time_ms": elapsed
        }
        
    except Exception as e:
        logger.error(f"‚ùå AI generation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AI generation failed: {str(e)}")

@app.post("/ai/generate/app", response_model=MultiFileAppResponse)
async def generate_full_app(
    request: MultiFileAppRequest,
    x_api_key: Optional[str] = Header(None),
    x_user_id: Optional[str] = Header(None),
    x_user_tier: Optional[str] = Header("free")
):
    """
    Generate complete application with multiple files
    This is the MAIN endpoint for NexusAI
    """
    start_time = time.time()
    user_id = x_user_id or "anonymous"
    
    # Rate limiting
    if not check_rate_limit(user_id, x_user_tier, "ai"):
        raise HTTPException(
            status_code=429,
            detail=f"Rate limit exceeded for {x_user_tier} tier"
        )
    
    logger.info(f"üé® Generating {request.framework.value} app for user {user_id}")
    logger.info(f"üìù Description: {request.description[:100]}...")
    
    # Check cache
    cache_key_val = cache_key("app", request.description, request.framework, request.features)
    cached = get_cache(cache_key_val)
    if cached:
        logger.info("üíæ Returning cached app generation")
        return MultiFileAppResponse(**cached)
    
    # Choose AI provider
    provider_name, client = choose_ai_provider(request.description, request.provider)
    
    # Generate prompt
    prompt = get_app_generation_prompt(request)
    
    try:
        # Call AI
        if provider_name == "openai":
            model = _default_openai_model()  # Best for code generation
            response = await client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are an expert full-stack developer. Generate complete, production-ready code with no placeholders. Return valid JSON only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=16000,
                response_format={"type": "json_object"}
            )
            result_text = response.choices[0].message.content
            tokens = response.usage.total_tokens if response.usage else 0
            
        else:  # anthropic
            model = _default_anthropic_model()
            max_tokens = _get_model_max_tokens("anthropic", model)
            response = await client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=0.7,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "You are an expert full-stack developer. Generate complete, production-ready code with no placeholders. Return valid JSON only."},
                            {"type": "text", "text": prompt}
                        ]
                    }
                ]
            )
            result_text = response.content[0].text
            tokens = response.usage.input_tokens + response.usage.output_tokens
        
        # Parse AI response with sanitization
        result_json = sanitize_and_parse_json(result_text, "generate_app")
        
        elapsed = int((time.time() - start_time) * 1000)
        
        # Build response
        response_data = { "files": result_json.get("files", []),
            "instructions": result_json.get("instructions", "No instructions provided"),
            "dependencies": result_json.get("dependencies", {}),
            "requires_database": result_json.get("requires_database", request.include_database),
            "database_schema": result_json.get("database_schema"),
            "deployment_config": {
                "framework": request.framework.value,
                "port": 3000,
                "build_command": "npm run build",
                "start_command": "npm start"
            },
            "provider_used": f"{provider_name}/{model}",
            "generation_time_ms": elapsed,
            "tokens_used": tokens
        }
        
        # Cache result
        set_cache(cache_key_val, response_data, ttl=3600)
        
        # Send N8N webhook (async, non-blocking)
        asyncio.create_task(send_n8n_webhook(N8N_APP_GENERATED, {
            "event": "app_generated",
            "user_id": user_id,
            "app_id": cache_key_val,
            "framework": request.framework.value,
            "provider": provider_name,
            "files_count": len(response_data["files"]),
            "requires_database": response_data["requires_database"],
            "generated_at": datetime.utcnow().isoformat()
        }))
        
        logger.info(f"‚úÖ App generated successfully: {len(response_data['files'])} files in {elapsed}ms using {provider_name}")
        
        return MultiFileAppResponse(**response_data)
        
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå Failed to parse AI response as JSON: {str(e)}")
        logger.error(f"Raw response: {result_text[:500]}...")
        raise HTTPException(
            status_code=500,
            detail="AI returned invalid JSON. Please try again."
        )
    except Exception as e:
        logger.error(f"‚ùå App generation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

@app.post("/deploy/app", response_model=Dict[str, Any])
async def deploy_app(
    request: DeploymentRequest,
    x_api_key: Optional[str] = Header(None),
    x_user_tier: Optional[str] = Header("free")
):
    """
    Deploy generated app
    Triggers N8N workflow for automated deployment
    """
    user_id = request.user_id
    
    # Rate limiting
    if not check_rate_limit(user_id, x_user_tier, "deploy"):
        raise HTTPException(
            status_code=429,
            detail=f"Deploy rate limit exceeded for {x_user_tier} tier"
        )
    
    import uuid

    deployment_id = str(uuid.uuid4())
    logger.info(f"üöÄ Deploying app '{request.app_name}' for user {user_id} (deployment_id={deployment_id})")
    
    # Track a minimal status for frontend polling (best-effort; per-worker memory).
    _deployment_status_store[deployment_id] = DeploymentStatusResponse(
        deployment_id=deployment_id,
        status="pending",
        progress=5,
        current_step="queued",
        logs=["Deployment queued"],
        error=None,
    )

    # Send to N8N for deployment automation
    deployment_payload = {
        "event": "deploy_app",
        "deployment_id": deployment_id,
        "app_id": request.app_id or deployment_id,
        "app_name": request.app_name,
        "user_id": user_id,
        "user_email": getattr(request, "user_email", None),
        "framework": request.framework,
        "files": [{"path": f.path, "content": f.content, "language": f.language} for f in request.files],
        "dependencies": request.dependencies,
        "requires_database": request.requires_database,
        "database_schema": request.database_schema,
        "requested_at": datetime.utcnow().isoformat()
    }
    
    # Send webhook to N8N (this will handle deployment)
    asyncio.create_task(send_n8n_webhook(N8N_APP_DEPLOY, deployment_payload))
    
    return {
        "deployment_id": deployment_id,
        "app_name": request.app_name,
        "status": "pending",
        "steps": [
            {"step": "queued", "status": "success"},
            {"step": "building", "status": "pending"},
            {"step": "deploying", "status": "pending"},
        ],
        "app_url": None,
    }

@app.post("/ai/deploy/app", response_model=Dict[str, Any])
async def deploy_app_ai_prefix(request: DeploymentRequest, x_api_key: Optional[str] = Header(None), x_user_tier: Optional[str] = Header("free")):
    """Alias for deployments when routed through /api/ai/* rewrite."""
    return await deploy_app(request, x_api_key=x_api_key, x_user_tier=x_user_tier)

@app.get("/deploy/status/{deployment_id}", response_model=DeploymentStatusResponse)
@app.get("/ai/deploy/status/{deployment_id}", response_model=DeploymentStatusResponse)
async def get_deployment_status(deployment_id: str):
    """Best-effort deployment status (front-end polling)."""
    status = _deployment_status_store.get(deployment_id)
    if status is None:
        return DeploymentStatusResponse(
            deployment_id=deployment_id,
            status="pending",
            progress=0,
            current_step="queued",
            logs=[],
            error=None,
        )
    return status

# ============================================
# ADVANCED DUAL-AI FULL-STACK GENERATION
# ============================================

@app.post("/ai/generate/fullstack", response_model=MultiFileAppResponse)
async def generate_fullstack_app(
    request: MultiFileAppRequest,
    x_api_key: Optional[str] = Header(None),
    x_user_id: Optional[str] = Header(None)
):
    """
    üöÄ ENTERPRISE-GRADE FULLSTACK GENERATION
    
    **Dual-AI Orchestration System (World's Most Advanced):**
    - Phase 1: Claude 3.5 Sonnet creates comprehensive architecture & database design
    - Phase 2: GPT-4o generates complete production-ready frontend code
    - Phase 3: GPT-4o generates backend API with all endpoints + Postman collection
    - Phase 4: Claude 3.5 Sonnet reviews, integrates, and optimizes everything
    - Phase 5: Automatic database provisioning with tables created in Database-as-a-Service
    
    **More Powerful Than Any Competitor:**
    - ‚úÖ Generates 20-40 complete files (vs Cursor: 5-10, Lovable: 8-12, Bolt: 10-15)
    - ‚úÖ Real backend API with auth, validation, error handling (competitors skip this)
    - ‚úÖ Complete database with indexes, constraints, migrations (competitors use fake data)
    - ‚úÖ Docker + deployment config + CI/CD ready (competitors provide basic setup)
    - ‚úÖ Postman collection for API testing (competitors don't include this)
    - ‚úÖ Production-grade code quality with NO placeholders! (competitors use TODOs)
    - ‚úÖ **AUTOMATIC DATABASE PROVISIONING** - Tables created instantly! (competitors don't do this)
    
    **Result:** Deploy-ready professional application in 30-45 seconds
    """
    start_time = time.time()
    user_id = x_user_id or "anonymous"
    
    # Must have at least OpenAI for generation
    if not openai_client:
        raise HTTPException(
            status_code=503,
            detail="Fullstack generation requires OpenAI API key (GPT-4o)"
        )
    
    # Claude enhances the generation but is not required
    has_anthropic = anthropic_client is not None
    
    try:
        logger.info(f"üéØ Starting DUAL-AI FULLSTACK generation: {request.description[:100]}")
        logger.info(f"ü§ñ Phase 1: Claude architects the system...")
        
        # ==============================================
        # PHASE 1: ARCHITECTURE & DATABASE DESIGN (Claude 3.5 Sonnet)
        # ==============================================
        architecture_prompt = f"""You are a senior software architect. Design a COMPLETE, production-grade full-stack application.

**Project:** {request.description}

**Tech Stack:**
- Frontend: {request.framework.value} + TypeScript + {request.styling.value}
- Backend: Node.js + Express + TypeScript
- Database: PostgreSQL with full schema
- Auth: JWT + bcrypt
- API: RESTful with OpenAPI docs
- Features: {', '.join(request .features) if request.features else 'Full CRUD + Auth + Admin'}

**Architecture Requirements:**
1. **Frontend Structure (15-20 files):**
   - Pages for all major features (Home, Dashboard, Login, Register, Profile, Admin, etc.)
   - Reusable UI components (buttons, forms, modals, cards, tables, navigation)
   - API client with interceptors and error handling
   - Authentication context and protected routes
   - State management (Context API or Redux)
   - Form validation with type safety
   - Responsive layouts and themes

2. **Backend API (10-15 files):**
   - Express server with middleware (cors, helmet, rate-limiting)
   - Authentication endpoints (register, login, refresh, logout)
   - CRUD endpoints for ALL resources
   - Input validation with express-validator
   - Error handling middleware
   - API documentation (Swagger/OpenAPI)
   - Database connection pooling
   - Environment configuration

3. **Database Design (5-10 tables):**
   - Users table (id, email, password_hash, role, created_at, updated_at)
   - Main resource tables with proper relations
   - Junction tables for many-to-many relationships  
   - Indexes on foreign keys and frequently queried columns
   - NOT NULL constraints and defaults
   - ON DELETE CASCADE for dependencies

4. **DevOps & Deployment:**
   - Dockerfile for frontend + backend
   - docker-compose.yml for local development
   - .env.example files
   - README with setup instructions
   - Postman collection with all API endpoints

**Output JSON Structure:**
{{
  "architecture": {{
    "frontend_pages": ["Home.tsx", "Dashboard.tsx", "Login.tsx", ...],
    "frontend_components": ["Button.tsx", "Modal.tsx", "Navbar.tsx", ...],
    "backend_routes": ["/api/auth/login", "/api/users", "/api/resources", ...],
    "database_tables": ["users", "resources", "permissions", ...]
  }},
  "database_schema": {{
    "tables": [
      {{
        "name": "users",
        "columns": [
          {{"name": "id", "type": "SERIAL PRIMARY KEY"}},
          {{"name": "email", "type": "VARCHAR(255) UNIQUE NOT NULL"}},
          {{"name": "password_hash", "type": "VARCHAR(255) NOT NULL"}},
          {{"name": "role", "type": "VARCHAR(50) DEFAULT 'user'"}},
          {{"name": "created_at", "type": "TIMESTAMP DEFAULT CURRENT_TIMESTAMP"}}
        ],
        "indexes": ["CREATE INDEX idx_users_email ON users(email);"]
      }},
      ...
    ]
  }},
  "api_endpoints": [
    {{"method": "POST", "path": "/api/auth/register", "description": "Create new user"}},
    {{"method": "POST", "path": "/api/auth/login", "description": "Login and get JWT"}},
    ...
  ],
  "file_list": ["frontend/src/pages/Home.tsx", "backend/src/routes/auth.ts", ...]
}}

**CRITICAL JSON FORMATTING RULES:**
- Properly escape ALL special characters in strings: \n for newlines, \t for tabs, \" for quotes
- No unescaped control characters (0x00-0x1F) in content  
- No actual newlines/tabs inside JSON strings - use \n and \t
- Validate JSON structure before returning
- Content field must be valid JSON string with escaped characters

Return ONLY valid JSON."""

        # Call AI for architecture (prefer Claude, fallback to GPT-4o)
        if has_anthropic:
            model = _default_anthropic_model()
            max_tokens = _get_model_max_tokens("anthropic", model)
            arch_response = await anthropic_client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=0.3,  # Low temp for structured thinking
                messages=[{
                    "role": "user",
                    "content": architecture_prompt
                }]
            )
            arch_text = arch_response.content[0].text.strip()
            arch_tokens = arch_response.usage.input_tokens + arch_response.usage.output_tokens
        else:
            # Use GPT-4o for architecture (reliable fallback)
            arch_response = await openai_client.chat.completions.create(
                model=_default_openai_model(),
                messages=[
                    {"role": "system", "content": "You are a senior software architect. Design complete production systems."},
                    {"role": "user", "content": architecture_prompt}
                ],
                temperature=0.3,
                max_tokens=16000,
                response_format={"type": "json_object"}
            )
            arch_text = arch_response.choices[0].message.content
            arch_tokens = arch_response.usage.total_tokens if arch_response.usage else 0
        
        architecture = sanitize_and_parse_json(arch_text, "Phase 1: Architecture")
        logger.info(f"‚úÖ Phase 1 complete: Architecture designed with {len(architecture.get('file_list', []))} files planned")
        
        # ==============================================
        # PHASE 2: FRONTEND CODE GENERATION (GPT-4o)
        # ==============================================
        logger.info(f"üé® Phase 2: GPT-4o generates frontend code...")
        
        frontend_files = architecture['architecture']['frontend_pages'] + architecture['architecture']['frontend_components']
        
        frontend_prompt = f"""Generate COMPLETE, production-ready {request.framework.value} frontend code.

**Architecture:** {json.dumps(architecture['architecture'], indent=2)}

**Generate ALL these files with full implementation:**
{json.dumps(frontend_files, indent=2)}

**Code Requirements:**
- TypeScript with strict typing
- {request.styling.value} for styling
- Axios for API calls with interceptors
- React Router for navigation
- JWT token management in localStorage
- Protected route components
- Form validation with error display
- Loading states and error boundaries
- Responsive design (mobile-first)
- Dark mode support
- NO PLACEHOLDERS - full working code

**Output JSON:**
{{
  "files": [
    {{"path": "frontend/src/pages/Home.tsx", "content": "Full TypeScript code here", "language": "typescript"}},
    {{"path": "frontend/src/components/Button.tsx", "content": "...", "language": "typescript"}},
    ...
  ]
}}

**CRITICAL JSON FORMATTING RULES:**
- Properly escape ALL special characters: \\n for newlines, \\t for tabs, \\\" for quotes, \\\\ for backslashes
- No unescaped control characters in \"content\" field
- Code must be valid JSON string with proper escaping
- Use \\n instead of actual newlines in code content
- Validate that output is parseable JSON before returning

Return ONLY valid JSON."""

        frontend_response = await openai_client.chat.completions.create(
            model=_default_openai_model(),
            messages=[
                {"role": "system", "content": "You are an expert frontend developer. Generate COMPLETE production code with no placeholders."},
                {"role": "user", "content": frontend_prompt}
            ],
            temperature=0.7,
            max_tokens=16000,
            response_format={"type": "json_object"}
        )
        
        frontend_text = frontend_response.choices[0].message.content
        frontend_data = sanitize_and_parse_json(frontend_text, "Phase 2: Frontend")
        all_files = frontend_data.get('files', [])
        logger.info(f"‚úÖ Phase 2 complete: {len(all_files)} frontend files generated")
        
        # ==============================================
        # PHASE 3: BACKEND API GENERATION (GPT-4o)
        # ==============================================
        logger.info(f"‚öôÔ∏è Phase 3: GPT-4o generates backend API...")
        
        backend_prompt = f"""Generate COMPLETE Express.js + TypeScript backend API.

**Architecture:** {json.dumps(architecture['architecture'], indent=2)}
**Database Schema:** {json.dumps(architecture['database_schema'], indent=2)}
**API Endpoints:** {json.dumps(architecture['api_endpoints'], indent=2)}

**Generate ALL backend files:**
- src/index.ts (Express server setup)
- src/routes/*.ts (All API routes)
- src/middleware/auth.ts (JWT verification)
- src/middleware/validation.ts (Request validation)
- src/middleware/errorHandler.ts (Error handling)
- src/config/database.ts (PostgreSQL connection pool)
- src/controllers/*.ts (Business logic)
- src/models/*.ts (TypeScript interfaces)

**Code Requirements:**
- Full CRUD operations for all resources
- JWT authentication with refresh tokens
- Password hashing with bcrypt
- Input validation with express-validator
- Proper error handling and status codes
- PostgreSQL with pg library and connection pooling
- Environment variables
- CORS, helmet, rate-limiting
- API documentation comments
- NO PLACEHOLDERS - production-ready code

**Output JSON:**
{{
  "files": [
    {{"path": "backend/src/index.ts", "content": "Full TypeScript code", "language": "typescript"}},
    {{"path": "backend/src/routes/auth.ts", "content": "...", "language": "typescript"}},
    ...
  ]
}}

**CRITICAL JSON FORMATTING RULES:**
- Properly escape ALL special characters: \\n for newlines, \\t for tabs, \\\" for quotes, \\\\ for backslashes
- No unescaped control characters (tabs, newlines) in content strings
- Code in \"content\" must be properly escaped as JSON string
- Use \\n instead of actual newlines
- Verify JSON is parseable before returning

Return ONLY valid JSON."""

        backend_response = await openai_client.chat.completions.create(
            model=_default_openai_model(),
            messages=[
                {"role": "system", "content": "You are an expert backend developer. Generate COMPLETE production API code with no placeholders."},
                {"role": "user", "content": backend_prompt}
            ],
            temperature=0.7,
            max_tokens=16000,
            response_format={"type": "json_object"}
        )
        
        backend_text = backend_response.choices[0].message.content
        backend_data = sanitize_and_parse_json(backend_text, "Phase 3: Backend")
        all_files.extend(backend_data.get('files', []))
        logger.info(f"‚úÖ Phase 3 complete: {len(backend_data.get('files', []))} backend files generated")
        
        # ==============================================
        # PHASE 4: INTEGRATION & OPTIMIZATION
        # ==============================================
        logger.info(f"üîß Phase 4: {'Claude' if has_anthropic else 'GPT-4o'} integrates and adds deployment files...")
        
        integration_prompt = f"""Review and complete the full-stack application with deployment files.

**Generated Files So Far:** {len(all_files)} files

**Add these essential files:**
1. frontend/package.json (complete dependencies)
2. backend/package.json (complete dependencies)
3. database/schema.sql (full CREATE TABLE statements)
4. frontend/Dockerfile
5. backend/Dockerfile
6. docker-compose.yml (frontend + backend + postgres)
7. .env.example (frontend + backend)
8. README.md (complete setup guide)
9. postman_collection.json (all API endpoints)
10. frontend/tsconfig.json
11. backend/tsconfig.json
12. .gitignore
13. nginx.conf (optional reverse proxy)

**Database Schema SQL:**
{json.dumps(architecture['database_schema'], indent=2)}

**Generate ONLY these deployment/config files. Output JSON:**
{{
  "files": [
    {{"path": "frontend/package.json", "content": "...", "language": "json"}},
    {{"path": "database/schema.sql", "content": "CREATE TABLE statements", "language": "sql"}},
    ...
  ],
  "setup_instructions": "Detailed README content here",
  "dependencies": {{"frontend": {{}}, "backend": {{}}}}
}}

**CRITICAL JSON FORMATTING RULES:**
- Properly escape ALL special characters in file content: \\n for newlines, \\t for tabs, \\" for quotes, \\\\ for backslashes
- SQL CREATE TABLE statements must use \\n instead of actual line breaks
- package.json and config files must have properly escaped content strings
- README content must escape all markdown special characters
- Validate that your entire output is valid, parseable JSON before returning

**CRITICAL JSON FORMATTING RULES:**
- Properly escape ALL special characters: \n for newlines, \t for tabs, \" for quotes, \\ for backslashes
- No unescaped control characters in any string fields
- SQL, config files, and code must be properly escaped in "content" fields
- Use \n instead of actual line breaks
- Validate JSON parseability before returning

Return ONLY valid JSON."""

        if has_anthropic:
            model = _default_anthropic_model()
            max_tokens = _get_model_max_tokens("anthropic", model)
            integration_response = await anthropic_client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=0.5,
                messages=[{
                    "role": "user",
                    "content": integration_prompt
                }]
            )
            integration_text = integration_response.content[0].text.strip()
            integration_tokens = integration_response.usage.input_tokens + integration_response.usage.output_tokens
        else:
            integration_response = await openai_client.chat.completions.create(
                model=_default_openai_model(),
                messages=[
                    {"role": "system", "content": "You are an expert DevOps engineer. Generate complete deployment configurations."},
                    {"role": "user", "content": integration_prompt}
                ],
                temperature=0.5,
                max_tokens=16000,
                response_format={"type": "json_object"}
            )
            integration_text = integration_response.choices[0].message.content
            integration_tokens = integration_response.usage.total_tokens if integration_response.usage else 0
        
        integration_data = sanitize_and_parse_json(integration_text, "Phase 4: Integration")
        all_files.extend(integration_data.get('files', []))
        logger.info(f"‚úÖ Phase 4 complete: {len(integration_data.get('files', []))} deployment files added")
        
        # Calculate total tokens
        total_tokens = (
            arch_tokens +
            frontend_response.usage.total_tokens +
            backend_response.usage.total_tokens +
            integration_tokens
        )
        
        elapsed = int((time.time() - start_time) * 1000)
        logger.info(f"üéâ FULLSTACK GENERATION COMPLETE: {len(all_files)} files in {elapsed}ms")
        
        # Extract database schema SQL
        schema_file = next((f for f in all_files if 'schema.sql' in f['path']), None)
        database_schema = schema_file['content'] if schema_file else ""
        
        # ==============================================
        # PHASE 5: SAVE APP & AUTOMATIC DATABASE PROVISIONING
        # ==============================================
        database_info = None
        app_id = None
        
        if user_id != "anonymous" and http_client:
            try:
                logger.info(f"üíæ Phase 5: Saving app and provisioning database...")
                
                # Step 1: Save the generated app to Node API
                import uuid
                app_id = str(uuid.uuid4())
                app_name = request.description[:50].replace(" ", "-").lower()
                
                save_app_url = f"{API_BASE_URL}/v1/generated-apps"
                save_app_payload = {
                    "id": app_id,
                    "app_name": app_name,
                    "description": request.description,
                    "framework": request.framework.value,
                    "styling": request.styling.value,
                    "features": request.features or [],
                    "files": [
                        {
                            "file_path": f["path"],
                            "content": f["content"],
                            "language": f.get("language", "typescript")
                        }
                        for f in all_files
                    ],
                    "dependencies": integration_data.get('dependencies', {}),
                    "requires_database": True,
                    "status": "complete"
                }
                
                try:
                    logger.info(f"   ‚Üí Saving app with {len(all_files)} files...")
                    save_response = await http_client.post(
                        save_app_url,
                        json=save_app_payload,
                        timeout=30.0
                    )
                    
                    if save_response.status_code in [200, 201]:
                        logger.info(f"   ‚úÖ App saved successfully: {app_id}")
                        
                        # Step 2: Provision database for the app
                        if database_schema:
                            logger.info(f"   ‚Üí Provisioning database with automatic schema execution...")
                            provision_url = f"{API_BASE_URL}/v1/generated-apps/{app_id}/database/provision"
                            
                            provision_response = await http_client.post(
                                provision_url,
                                json={"initialize_schema": True},
                                timeout=20.0
                            )
                            
                            if provision_response.status_code == 200:
                                provision_data = provision_response.json()
                                database_info = provision_data.get('database', {})
                                tables_created = provision_data.get('tables_created', 0)
                                
                                logger.info(f"   ‚úÖ Database provisioned: {database_info.get('database', 'unknown')} with {tables_created} tables created!")
                                
                                # Add database connection info to response
                                database_info['tables_created'] = tables_created
                                database_info['connection_string'] = provision_data.get('connection_string', '')
                            else:
                                logger.warning(f"   ‚ö†Ô∏è  Database provisioning returned {provision_response.status_code}: {provision_response.text[:200]}")
                    else:
                        logger.warning(f"   ‚ö†Ô∏è  App save returned {save_response.status_code}: {save_response.text[:200]}")
                
                except Exception as api_error:
                    logger.error(f"   ‚ùå API call failed: {str(api_error)}")
                    # Continue without saving - user gets files anyway
                
            except Exception as provision_error:
                logger.error(f"Phase 5 error (non-fatal): {str(provision_error)}")
                # Continue - database provisioning is optional enhancement
        
        # Build response
        response_data = {
            "files": all_files,
            "instructions": integration_data.get('setup_instructions', 'See README.md for setup instructions'),
            "dependencies": integration_data.get('dependencies', {}),
            "requires_database": True,
            "database_schema": database_schema,
            "database_info": database_info,  # Include provisioned database credentials
            "app_id": app_id,  # Include app ID if saved
            "deployment_config": {
                "framework": request.framework.value,
                "port": 3000,
                "build_command": "npm run build",
                "start_command": "npm start"
            },
            "provider_used": "dual-ai/claude-3.5-sonnet+gpt-4o" if has_anthropic else "openai/gpt-4o",
            "generation_time_ms": elapsed,
            "tokens_used": total_tokens
        }
        
        # Send N8N webhook (async, non-blocking)
        asyncio.create_task(send_n8n_webhook(N8N_APP_GENERATED, {
            "event": "fullstack_app_generated",
            "user_id": user_id,
            "app_id": app_id,
            "framework": request.framework.value,
            "provider": "dual-ai" if has_anthropic else "openai",
            "files_count": len(response_data["files"]),
            "requires_database": response_data["requires_database"],
            "database_provisioned": database_info is not None,
            "tables_created": database_info.get('tables_created', 0) if database_info else 0,
            "generated_at": datetime.utcnow().isoformat()
        }))
        
        logger.info(f"üéâ FULLSTACK APP COMPLETE: {len(response_data['files'])} files | Database: {'‚úÖ PROVISIONED with ' + str(database_info.get('tables_created', 0)) + ' tables' if database_info else 'üìÑ Schema only'}")
        
        return MultiFileAppResponse(**response_data)
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON parsing error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AI returned invalid JSON: {str(e)}")
    except Exception as e:
        logger.error(f"Fullstack generation error: {str(e)}", exc_info=True)
        
        # Send error webhook
        if N8N_APP_ERROR:
            asyncio.create_task(send_n8n_webhook(N8N_APP_ERROR, {
                "error": str(e),
                "description": request.description,
                "timestamp": datetime.utcnow().isoformat()
            }))
        
        raise HTTPException(status_code=500, detail=str(e))

# ============================================
# ASYNC FULLSTACK GENERATION (Job Queue)
# ============================================

async def _execute_fullstack_generation_background(
    job_id: str,
    request: MultiFileAppRequest,
    user_id: str
):
    """
    Background task: Execute 5-phase fullstack generation with progress updates
    This runs asynchronously and updates job status in Redis
    """
    try:
        start_time = time.time()
        
        # Must have at least OpenAI
        if not openai_client:
            await fail_job(job_id, "Fullstack generation requires OpenAI API key (GPT-4o)")
            return
        
        has_anthropic = anthropic_client is not None
        
        logger.info(f"üéØ JOB {job_id}: Starting DUAL-AI FULLSTACK generation")
        
        # ==============================================
        # PHASE 1: ARCHITECTURE & DATABASE DESIGN (20%)
        # ==============================================
        await update_job_progress(job_id, JobPhase.ARCHITECTURE, 10, "Designing system architecture with Claude 3.5 Sonnet...")
        
        architecture_prompt = f"""You are a senior software architect. Design a COMPLETE, production-grade full-stack application.

**Project:** {request.description}
**Framework:** {request.framework.value}
**Features:** {', '.join(request.features) if request.features else 'Standard CRUD operations'}
**Requires Auth:** {"Yes" if request.include_auth else "No"}
**Requires API:** {"Yes" if request.include_api else "No"}

Design:
1. System architecture (frontend, backend, database layers)
2. Complete database schema with all tables, columns, indexes, constraints
3. API endpoints (REST) with request/response schemas
4. Frontend component hierarchy

**CRITICAL JSON FORMATTING RULES:**
- Properly escape ALL special characters: \\n for newlines, \\t for tabs, \\" for quotes, \\\\ for backslashes
- No unescaped control characters in "description" field
- Validate JSON parseability before returning

Return ONLY valid JSON:
{{
  "architecture": {{"description": "...", "layers": [...]}},
  "database_schema": {{"tables": [...], "indexes": [...], "relationships": [...]}},
  "api_endpoints": [...],
  "frontend_components": [...]
}}"""

        if has_anthropic:
            model = _default_anthropic_model()
            max_tokens = _get_model_max_tokens("anthropic", model)
            architecture_response = await anthropic_client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=0.5,
                messages=[{"role": "user", "content": architecture_prompt}]
            )
            architecture_text = architecture_response.content[0].text.strip()
            architecture_tokens = architecture_response.usage.input_tokens + architecture_response.usage.output_tokens
        else:
            architecture_response = await openai_client.chat.completions.create(
                model=_default_openai_model(),
                messages=[
                    {"role": "system", "content": "You are a senior software architect."},
                    {"role": "user", "content": architecture_prompt}
                ],
                temperature=0.5
            )
            architecture_text = architecture_response.choices[0].message.content.strip()
            architecture_tokens = architecture_response.usage.total_tokens
        
        architecture = sanitize_and_parse_json(architecture_text, "Phase 1: Architecture")
        database_schema = architecture.get('database_schema', {})
        
        await update_job_progress(job_id, JobPhase.ARCHITECTURE, 20, "‚úÖ Architecture designed")
        
        # ==============================================
        # PHASE 2: FRONTEND GENERATION (40%)
        # ==============================================
        await update_job_progress(job_id, JobPhase.FRONTEND, 25, "Generating frontend code with GPT-4o...")
        
        frontend_prompt = f"""Generate a COMPLETE, production-ready {request.framework.value} frontend application.

**Architecture:** {json.dumps(architecture['architecture'], indent=2)}
**Database Schema:** {json.dumps(database_schema, indent=2)}

**Generate ALL frontend files:**
- src/App.jsx (main app with routing)
- src/components/* (all UI components based on database entities)
- src/pages/* (CRUD views for all entities)
- src/services/api.js (API client with all endpoints)
- src/utils/auth.js (if auth required)
- public/index.html
- tailwind.config.js, postcss.config.js
- package.json (with ALL dependencies)

**CRITICAL JSON FORMATTING RULES:**
- Properly escape ALL special characters: \\n for newlines, \\t for tabs, \\" for quotes, \\\\ for backslashes
- No unescaped control characters in "content" field
- Code must be valid JSON string with proper escaping  
- Use \\n instead of actual newlines in code content
- Validate that output is parseable JSON before returning

Output JSON:
{{
  "files": [
    {{"path": "src/App.jsx", "content": "...", "language": "javascript"}},
    ...
  ]
}}"""

        frontend_response = await openai_client.chat.completions.create(
            model=_default_openai_model(),
            messages=[
                {"role": "system", "content": "You are an expert frontend developer."},
                {"role": "user", "content": frontend_prompt}
            ],
            temperature=0.6
        )
        frontend_text = frontend_response.choices[0].message.content.strip()
        frontend_tokens = frontend_response.usage.total_tokens
        frontend_data = sanitize_and_parse_json(frontend_text, "Phase 2: Frontend")
        frontend_files = frontend_data.get('files', [])
        
        await update_job_progress(job_id, JobPhase.FRONTEND, 40, f"‚úÖ Generated {len(frontend_files)} frontend files")
        
        # ==============================================
        # PHASE 3: BACKEND GENERATION (60%)
        # ==============================================
        await update_job_progress(job_id, JobPhase.BACKEND, 45, "Generating backend API with GPT-4o...")
        
        backend_prompt = f"""Generate a COMPLETE, production-ready backend API.

**Architecture:** {json.dumps(architecture['architecture'], indent=2)}
**Database Schema:** {json.dumps(database_schema, indent=2)}
**API Endpoints:** {json.dumps(architecture.get('api_endpoints', []), indent=2)}

**Generate ALL backend files:**
- server.js or app.py (Express/FastAPI/Flask server)
- routes/* (all API endpoints)
- models/* (database models for all entities)
- middleware/auth.js (JWT authentication)
- middleware/validation.js
- middleware/errorHandler.js
- config/database.js
- .env.example
- package.json or requirements.txt

**CRITICAL JSON FORMATTING RULES:**
- Properly escape ALL special characters: \\n for newlines, \\t for tabs, \\" for quotes, \\\\ for backslashes
- No unescaped control characters in "content" field
- Code must be valid JSON string with proper escaping
- Use \\n instead of actual newlines in code content
- Validate that output is parseable JSON before returning

Output JSON:
{{
  "files": [
    {{"path": "server.js", "content": "...", "language": "javascript"}},
    ...
  ]
}}"""

        backend_response = await openai_client.chat.completions.create(
            model=_default_openai_model(),
            messages=[
                {"role": "system", "content": "You are an expert backend developer."},
                {"role": "user", "content": backend_prompt}
            ],
            temperature=0.6
        )
        backend_text = backend_response.choices[0].message.content.strip()
        backend_tokens = backend_response.usage.total_tokens
        backend_data = sanitize_and_parse_json(backend_text, "Phase 3: Backend")
        backend_files = backend_data.get('files', [])
        
        await update_job_progress(job_id, JobPhase.BACKEND, 60, f"‚úÖ Generated {len(backend_files)} backend files")
        
        # ==============================================
        # PHASE 4: INTEGRATION FILES (80%)
        # ==============================================
        await update_job_progress(job_id, JobPhase.INTEGRATION, 65, "Generating deployment configs and documentation...")
        
        integration_prompt = f"""Generate deployment configs and integration files.

**Database Schema:** {json.dumps(database_schema, indent=2)}

**Generate ONLY these deployment/config files:**
1. frontend/package.json
2. backend/package.json or requirements.txt
3. database/schema.sql (CREATE TABLE statements)
4. README.md (complete setup guide)
5. postman_collection.json (all API endpoints)
6. docker-compose.yml (optional)
7. .env.example
8. .gitignore

**CRITICAL JSON FORMATTING RULES:**
- Properly escape ALL special characters in file content: \\n for newlines, \\t for tabs, \\" for quotes, \\\\ for backslashes
- SQL CREATE TABLE statements must use \\n instead of actual line breaks
- package.json and config files must have properly escaped content strings
- README content must escape all markdown special characters
- Validate that your entire output is valid, parseable JSON before returning

Output JSON:
{{
  "files": [
    {{"path": "database/schema.sql", "content": "CREATE TABLE...", "language": "sql"}},
    ...
  ],
  "setup_instructions": "...",
  "dependencies": {{"frontend": {{}}, "backend": {{}}}}
}}"""

        if has_anthropic:
            model = _default_anthropic_model()
            max_tokens = _get_model_max_tokens("anthropic", model)
            integration_response = await anthropic_client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=0.5,
                messages=[{"role": "user", "content": integration_prompt}]
            )
            integration_text = integration_response.content[0].text.strip()
            integration_tokens = integration_response.usage.input_tokens + integration_response.usage.output_tokens
        else:
            integration_response = await openai_client.chat.completions.create(
                model=_default_openai_model(),
                messages=[
                    {"role": "system", "content": "You are an expert DevOps engineer."},
                    {"role": "user", "content": integration_prompt}
                ],
                temperature=0.5
            )
            integration_text = integration_response.choices[0].message.content.strip()
            integration_tokens = integration_response.usage.total_tokens
        
        integration_data = sanitize_and_parse_json(integration_text, "Phase 4: Integration")
        integration_files = integration_data.get('files', [])
        
        await update_job_progress(job_id, JobPhase.INTEGRATION, 80, f"‚úÖ Generated {len(integration_files)} config files")
        
        # ==============================================
        # PHASE 5: DATABASE PROVISIONING (100%)
        # ==============================================
        await update_job_progress(job_id, JobPhase.DATABASE, 85, "Provisioning database and saving app...")
        
        all_files = frontend_files + backend_files + integration_files
        total_tokens = architecture_tokens + frontend_tokens + backend_tokens + integration_tokens
        elapsed = int((time.time() - start_time) * 1000)
        
        database_info = None
        app_id = None
        
        if request.include_database and database_schema:
            try:
                provision_payload = {
                    "user_id": user_id,
                    "app_name": f"nexusai_{user_id}_{int(time.time())}",
                    "database_schema": database_schema
                }
                
                async with httpx.AsyncClient(timeout=30.0) as client:
                    resp = await client.post(
                        f"{DATABASE_PROVISIONER_URL}/provision",
                        json=provision_payload
                    )
                    
                    if resp.status_code == 200:
                        database_info = resp.json()
                        logger.info(f"   ‚úÖ Database provisioned: {database_info.get('database')} with {database_info.get('tables_created', 0)} tables")
                    else:
                        logger.warning(f"   ‚ö†Ô∏è  Database provisioning failed: {resp.status_code}")
                
                # Save app to database
                try:
                    app_payload = {
                        "name": provision_payload["app_name"],
                        "description": request.description,
                        "framework": request.framework.value,
                        "files": all_files,
                        "dependencies": integration_data.get('dependencies', {}),
                        "database_schema": database_schema,
                        "user_id": user_id
                    }
                    
                    async with httpx.AsyncClient(timeout=10.0) as client:
                        save_resp = await client.post(
                            f"{API_BASE_URL}/generated-apps",
                            json=app_payload
                        )
                        
                        if save_resp.status_code == 200:
                            app_data = save_resp.json()
                            app_id = app_data.get('id')
                            logger.info(f"   ‚úÖ App saved to database: ID {app_id}")
                
                except Exception as api_error:
                    logger.error(f"   ‚ùå API save failed: {str(api_error)}")
            
            except Exception as provision_error:
                logger.error(f"Phase 5 error (non-fatal): {str(provision_error)}")
        
        await update_job_progress(job_id, JobPhase.DATABASE, 95, "Finalizing generation...")
        
        # Build final result
        result = {
            "files": all_files,
            "instructions": integration_data.get('setup_instructions', 'See README.md'),
            "dependencies": integration_data.get('dependencies', {}),
            "requires_database": True,
            "database_schema": database_schema,
            "database_info": database_info,
            "app_id": app_id,
            "deployment_config": {
                "framework": request.framework.value,
                "port": 3000,
                "build_command": "npm run build",
                "start_command": "npm start"
            },
            "provider_used": "dual-ai/claude-3.5-sonnet+gpt-4o" if has_anthropic else "openai/gpt-4o",
            "generation_time_ms": elapsed,
            "tokens_used": total_tokens
        }
        
        # Mark job complete
        await complete_job(job_id, result)
        
        # Send N8N webhook
        asyncio.create_task(send_n8n_webhook(N8N_APP_GENERATED, {
            "event": "fullstack_app_generated",
            "job_id": job_id,
            "user_id": user_id,
            "app_id": app_id,
            "framework": request.framework.value,
            "files_count": len(all_files),
            "database_provisioned": database_info is not None,
            "generated_at": datetime.utcnow().isoformat()
        }))
        
        logger.info(f"üéâ JOB {job_id} COMPLETE: {len(all_files)} files, {elapsed}ms")
    
    except Exception as e:
        logger.error(f"‚ùå JOB {job_id} FAILED: {str(e)}", exc_info=True)
        await fail_job(job_id, str(e))
        
        # Send error webhook
        asyncio.create_task(send_n8n_webhook(N8N_APP_ERROR, {
            "error": str(e),
            "job_id": job_id,
            "timestamp": datetime.utcnow().isoformat()
        }))


@app.post("/ai/generate/fullstack/async")
async def generate_fullstack_app_async(
    request: MultiFileAppRequest,
    x_api_key: Optional[str] = Header(None),
    x_user_id: Optional[str] = Header(None)
):
    """
    üöÄ ASYNC FULLSTACK GENERATION (Enterprise Job Queue)
    
    Returns job_id immediately. Poll GET /ai/jobs/{job_id} for progress and result.
    
    **Benefits:**
    - ‚úÖ No timeouts (generation runs in background)
    - ‚úÖ Real-time progress updates
    - ‚úÖ User can navigate away and come back
    - ‚úÖ Handles concurrent generations
    """
    user_id = x_user_id or "anonymous"
    
    # Create job and return immediately
    job_id = await create_job(user_id, request.dict())
    
    # Start background task
    asyncio.create_task(_execute_fullstack_generation_background(job_id, request, user_id))
    
    logger.info(f"‚úÖ Job {job_id} created for user {user_id}")
    
    return {
        "job_id": job_id,
        "status": "pending",
        "message": "Fullstack generation started. Poll /ai/jobs/{job_id} for progress.",
        "poll_url": f"/ai/jobs/{job_id}"
    }


@app.get("/ai/jobs/{job_id}", response_model=JobInfo)
async def get_job(job_id: str):
    """
    üìä GET JOB STATUS (Polling Endpoint)
    
    Returns current job status, progress, and result if completed.
    Frontend should poll this every 2-3 seconds.
    """
    job_info = await get_job_status(job_id)
    
    if not job_info:
        raise HTTPException(
            status_code=404,
            detail=f"Job {job_id} not found. Jobs expire after 24 hours."
        )
    
    return job_info

# ============================================
# ERROR HANDLERS
# ============================================

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handle HTTP exceptions"""
    logger.error(f"HTTP {exc.status_code}: {exc.detail}")
    return JSONResponse(
        status_code=exc.status_code,
        content={"error": exc.detail}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """Handle general exceptions"""
    logger.error(f"Unhandled exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"error": "Internal server error"}
    )

# ============================================
# RUN SERVER
# ============================================

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "5001"))
    uvicorn.run(
        "app_nexusai_production:app",
        host="0.0.0.0",
        port=port,
        reload=True,
        log_level="info"
    )
